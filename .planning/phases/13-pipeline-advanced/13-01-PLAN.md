---
phase: 13-pipeline-advanced
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/editorial_ai/routing/__init__.py
  - src/editorial_ai/routing/model_router.py
  - src/editorial_ai/routing/routing_config.yaml
  - src/editorial_ai/config.py
  - src/editorial_ai/observability/models.py
  - src/editorial_ai/observability/collector.py
  - src/editorial_ai/services/curation_service.py
  - src/editorial_ai/services/editorial_service.py
  - src/editorial_ai/services/review_service.py
  - src/editorial_ai/services/enrich_service.py
  - src/editorial_ai/services/design_spec_service.py
  - tests/test_model_router.py
autonomous: true

must_haves:
  truths:
    - "ModelRouter resolves node_name + context (revision_count) to a Gemini model name from YAML config"
    - "Flash-Lite is assigned to simple nodes (subtopics, extract, design_spec, layout_parse, repair, keyword_expand), Flash to complex nodes (curation_research, editorial_content, enrich_regenerate, review)"
    - "Upgrade conditions trigger Pro model when revision_count >= 2 for editorial_content and review nodes"
    - "Default fallback is gemini-2.5-flash when node is not in config"
    - "TokenUsage model includes routing_reason field for observability logging"
    - "record_token_usage accepts and stores routing_reason"
    - "All 10+ LLM call sites pass model from router instead of settings.default_model/editorial_model"
    - "Routing decisions are logged in token usage with routing_reason (e.g., 'default', 'upgrade:revision>=2')"
  artifacts:
    - path: "src/editorial_ai/routing/__init__.py"
      provides: "Package exports for ModelRouter and get_model_router"
    - path: "src/editorial_ai/routing/model_router.py"
      provides: "ModelRouter class with resolve() method and module-level get_model_router() singleton"
      contains: "class ModelRouter"
    - path: "src/editorial_ai/routing/routing_config.yaml"
      provides: "YAML config mapping node names to default/upgrade models with conditions"
      contains: "gemini-2.5-flash-lite"
    - path: "src/editorial_ai/observability/models.py"
      provides: "TokenUsage with routing_reason field"
      contains: "routing_reason"
    - path: "tests/test_model_router.py"
      provides: "Unit tests for ModelRouter config loading, resolve, upgrade conditions, fallback"
  key_links:
    - from: "src/editorial_ai/routing/model_router.py"
      to: "src/editorial_ai/routing/routing_config.yaml"
      via: "ModelRouter loads YAML config at init"
    - from: "src/editorial_ai/services/curation_service.py"
      to: "src/editorial_ai/routing/model_router.py"
      via: "Each LLM call resolves model via get_model_router().resolve()"
    - from: "src/editorial_ai/observability/collector.py"
      to: "src/editorial_ai/observability/models.py"
      via: "record_token_usage creates TokenUsage with routing_reason"
---

<objective>
Build a config-driven model router that maps pipeline node names to Gemini models (Pro/Flash/Flash-Lite), wire it into all LLM call sites, and log routing decisions in observability.

Purpose: Enables 3-10x cost reduction on simple tasks by routing to Flash-Lite, while preserving quality on complex tasks (Flash) with automatic Pro upgrade on retries.
Output: `src/editorial_ai/routing/` package + all services using router-resolved models + routing logged in token usage.
</objective>

<execution_context>
@/Users/kiyeol/.claude-pers/get-shit-done/workflows/execute-plan.md
@/Users/kiyeol/.claude-pers/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/13-pipeline-advanced/13-CONTEXT.md
@.planning/phases/13-pipeline-advanced/13-RESEARCH.md
@src/editorial_ai/config.py
@src/editorial_ai/services/curation_service.py
@src/editorial_ai/services/editorial_service.py
@src/editorial_ai/services/review_service.py
@src/editorial_ai/services/enrich_service.py
@src/editorial_ai/services/design_spec_service.py
@src/editorial_ai/observability/models.py
@src/editorial_ai/observability/collector.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install PyYAML + ModelRouter + YAML config</name>
  <files>
    pyproject.toml
    src/editorial_ai/routing/__init__.py
    src/editorial_ai/routing/model_router.py
    src/editorial_ai/routing/routing_config.yaml
  </files>
  <action>
**Step 0: Install PyYAML**
```bash
uv add pyyaml
```

**Step 1: Create `src/editorial_ai/routing/routing_config.yaml`:**

```yaml
defaults:
  model: "gemini-2.5-flash"

nodes:
  curation_research:
    default_model: "gemini-2.5-flash"
  curation_subtopics:
    default_model: "gemini-2.5-flash-lite"
  curation_extract:
    default_model: "gemini-2.5-flash-lite"
  design_spec:
    default_model: "gemini-2.5-flash-lite"
  editorial_content:
    default_model: "gemini-2.5-flash"
    upgrade_model: "gemini-2.5-pro"
    upgrade_conditions:
      min_revision_count: 2
  editorial_layout_parse:
    default_model: "gemini-2.5-flash-lite"
  editorial_repair:
    default_model: "gemini-2.5-flash-lite"
  enrich_keywords:
    default_model: "gemini-2.5-flash-lite"
  enrich_regenerate:
    default_model: "gemini-2.5-flash"
  review:
    default_model: "gemini-2.5-flash"
    upgrade_model: "gemini-2.5-pro"
    upgrade_conditions:
      min_revision_count: 2
```

**Step 2: Create `src/editorial_ai/routing/model_router.py`:**

```python
"""Config-driven model router for dynamic Gemini model selection.

Maps pipeline node names to Gemini models based on task complexity,
with conditional upgrade to higher-tier models on retries.
"""

import logging
from dataclasses import dataclass, field
from pathlib import Path

import yaml

logger = logging.getLogger(__name__)

_DEFAULT_CONFIG_PATH = Path(__file__).parent / "routing_config.yaml"


@dataclass
class ModelRoute:
    default_model: str
    upgrade_model: str | None = None
    upgrade_conditions: dict = field(default_factory=dict)


@dataclass
class RoutingDecision:
    model: str
    reason: str  # "default", "upgrade:revision>=2", "fallback"


class ModelRouter:
    """Resolves (node_name, context) -> model_name from YAML config."""

    def __init__(self, config_path: Path | str | None = None) -> None:
        path = Path(config_path) if config_path else _DEFAULT_CONFIG_PATH
        with open(path) as f:
            raw = yaml.safe_load(f)

        self._fallback_model = raw.get("defaults", {}).get("model", "gemini-2.5-flash")
        self._routes: dict[str, ModelRoute] = {}

        for node_name, cfg in raw.get("nodes", {}).items():
            self._routes[node_name] = ModelRoute(
                default_model=cfg["default_model"],
                upgrade_model=cfg.get("upgrade_model"),
                upgrade_conditions=cfg.get("upgrade_conditions", {}),
            )

    def resolve(
        self,
        node_name: str,
        *,
        revision_count: int = 0,
    ) -> RoutingDecision:
        """Resolve a model for the given node and context.

        Returns a RoutingDecision with model name and reason string.
        """
        route = self._routes.get(node_name)
        if not route:
            return RoutingDecision(model=self._fallback_model, reason="fallback")

        # Check upgrade conditions
        if route.upgrade_model and route.upgrade_conditions:
            min_rev = route.upgrade_conditions.get("min_revision_count")
            if min_rev is not None and revision_count >= min_rev:
                return RoutingDecision(
                    model=route.upgrade_model,
                    reason=f"upgrade:revision>={min_rev}",
                )

        return RoutingDecision(model=route.default_model, reason="default")

    @property
    def fallback_model(self) -> str:
        return self._fallback_model


# Module-level singleton
_router_instance: ModelRouter | None = None


def get_model_router() -> ModelRouter:
    """Get or create the singleton ModelRouter."""
    global _router_instance
    if _router_instance is None:
        _router_instance = ModelRouter()
    return _router_instance
```

**Step 3: Create `src/editorial_ai/routing/__init__.py`:**

```python
from editorial_ai.routing.model_router import (
    ModelRouter,
    RoutingDecision,
    get_model_router,
)

__all__ = ["ModelRouter", "RoutingDecision", "get_model_router"]
```
  </action>
  <verify>
```bash
python -c "from editorial_ai.routing import ModelRouter, get_model_router; r = get_model_router(); d = r.resolve('curation_subtopics'); print(f'{d.model} ({d.reason})')"
```
Expected output: `gemini-2.5-flash-lite (default)`

```bash
python -c "from editorial_ai.routing import get_model_router; r = get_model_router(); d = r.resolve('editorial_content', revision_count=3); print(f'{d.model} ({d.reason})')"
```
Expected output: `gemini-2.5-pro (upgrade:revision>=2)`
  </verify>
  <done>ModelRouter loads YAML config, resolves nodes to models, supports upgrade conditions.</done>
</task>

<task type="auto">
  <name>Task 2: Extend observability + wire all services to use router</name>
  <files>
    src/editorial_ai/observability/models.py
    src/editorial_ai/observability/collector.py
    src/editorial_ai/services/curation_service.py
    src/editorial_ai/services/editorial_service.py
    src/editorial_ai/services/review_service.py
    src/editorial_ai/services/enrich_service.py
    src/editorial_ai/services/design_spec_service.py
    tests/test_model_router.py
  </files>
  <action>
**Step 1: Extend `TokenUsage` in `observability/models.py`:**

Add `routing_reason: str | None = None` field to the `TokenUsage` model.

**Step 2: Extend `record_token_usage` in `observability/collector.py`:**

Add `routing_reason: str | None = None` parameter to `record_token_usage()`. Pass it to `TokenUsage(... routing_reason=routing_reason)`.

**Step 3: Wire `CurationService` to use model router.**

In `curation_service.py`:
- Add import: `from editorial_ai.routing import get_model_router`
- **`research_trend()`**: Resolve model via `get_model_router().resolve("curation_research")`. Use `decision.model` instead of `self.model`. Pass `routing_reason=decision.reason` to `record_token_usage`.
- **`expand_subtopics()`**: Resolve via `get_model_router().resolve("curation_subtopics")`. Same pattern.
- **`extract_topic()`**: Resolve via `get_model_router().resolve("curation_extract")`. Same pattern.

Each method currently uses `self.model` (which defaults to `settings.default_model`). Replace with the router-resolved model. Keep `self.model` as a fallback only if the router fails.

**Step 4: Wire `EditorialService` to use model router.**

In `editorial_service.py`:
- Add import: `from editorial_ai.routing import get_model_router`
- **`generate_content()`**: The service needs `revision_count` context. Add an optional `revision_count: int = 0` parameter to `generate_content()`. Resolve via `get_model_router().resolve("editorial_content", revision_count=revision_count)`. Use `decision.model` instead of `self.content_model`. Pass `routing_reason=decision.reason` to `record_token_usage`. Also update `create_editorial()` to accept and pass through `revision_count`.
- **`generate_layout_image()`**: Image model stays as `self.image_model` (separate model, not routed).
- **`parse_layout_image()`**: Resolve via `get_model_router().resolve("editorial_layout_parse")`.
- **`repair_output()`**: Resolve via `get_model_router().resolve("editorial_repair")`.

Update `editorial_node` (`nodes/editorial.py`) to pass `revision_count=state.get("revision_count", 0)` to `service.create_editorial()`.

**Step 5: Wire `ReviewService` to use model router.**

In `review_service.py`:
- Add import: `from editorial_ai.routing import get_model_router`
- **`evaluate_with_llm()`**: Add optional `revision_count: int = 0` parameter. Resolve via `get_model_router().resolve("review", revision_count=revision_count)`. Use `decision.model`. Pass `routing_reason=decision.reason` to `record_token_usage`.
- Update `evaluate()` to accept and pass through `revision_count`.

Update `review_node` (`nodes/review.py`) to pass `revision_count=state.get("revision_count", 0)` to `service.evaluate()`.

**Step 6: Wire `enrich_service.py` to use model router.**

- **`expand_keywords()`**: Resolve via `get_model_router().resolve("enrich_keywords")`. Use `decision.model` instead of `settings.default_model`.
- **`regenerate_with_enrichment()`**: Resolve via `get_model_router().resolve("enrich_regenerate")`. Use `decision.model` instead of `settings.editorial_model`.

**Step 7: Wire `DesignSpecService` to use model router.**

In `design_spec_service.py`:
- **`generate_spec()`**: Resolve via `get_model_router().resolve("design_spec")`. Use `decision.model` instead of `self.model_name`.

**Step 8: Write `tests/test_model_router.py`:**

Tests:
1. `test_resolve_default_model` — Flash-Lite nodes return correct model
2. `test_resolve_complex_model` — Flash nodes return correct model
3. `test_resolve_upgrade_condition` — revision_count >= 2 triggers Pro
4. `test_resolve_no_upgrade_below_threshold` — revision_count < 2 stays at Flash
5. `test_resolve_unknown_node_fallback` — unknown node returns fallback
6. `test_routing_decision_has_reason` — reason string populated correctly
7. `test_custom_config_path` — can load from custom path (create temp YAML)

Use pytest. Create a temp YAML config fixture for isolation from production config changes.
  </action>
  <verify>
1. `python -m pytest tests/test_model_router.py -v` — all tests pass
2. `grep -r "get_model_router" src/editorial_ai/services/` — shows router usage in all 5 service files
3. `grep -r "routing_reason" src/editorial_ai/` — shows in models.py, collector.py, and service files
4. `python -c "from editorial_ai.graph import build_graph; print('OK')"` — graph still compiles
  </verify>
  <done>All LLM call sites use router-resolved models. Routing decisions logged in token usage. Tests cover resolver logic.</done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_model_router.py -v` — all router tests pass
2. `python -c "from editorial_ai.graph import build_graph; g = build_graph(); print('Graph compiled OK')"` — no import errors
3. `python -c "from editorial_ai.routing import get_model_router; r = get_model_router(); print(r.resolve('curation_subtopics').model)"` — prints `gemini-2.5-flash-lite`
4. `python -c "from editorial_ai.routing import get_model_router; r = get_model_router(); print(r.resolve('review', revision_count=3).model)"` — prints `gemini-2.5-pro`
5. `grep -c "get_model_router" src/editorial_ai/services/*.py` — shows 5 files with router usage
6. `python -m pytest tests/ -x --timeout=30` — existing tests still pass (router resolves even without actual API keys)
</verification>

<success_criteria>
- ModelRouter resolves node names to correct models per YAML config
- Flash-Lite for simple tasks, Flash for complex, Pro on upgrade
- All 10+ LLM call sites use router-resolved models
- Routing decisions captured in TokenUsage.routing_reason
- Existing tests unbroken
- New router tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/13-pipeline-advanced/13-01-SUMMARY.md`
</output>
