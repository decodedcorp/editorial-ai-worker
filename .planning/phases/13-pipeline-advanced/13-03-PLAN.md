---
phase: 13-pipeline-advanced
plan: 03
type: execute
wave: 2
depends_on: ["13-01"]
files_modified:
  - src/editorial_ai/caching/__init__.py
  - src/editorial_ai/caching/cache_manager.py
  - src/editorial_ai/services/review_service.py
  - src/editorial_ai/services/editorial_service.py
  - src/editorial_ai/nodes/review.py
  - src/editorial_ai/nodes/editorial.py
  - src/editorial_ai/observability/models.py
  - tests/test_cache_manager.py
autonomous: true

must_haves:
  truths:
    - "CacheManager wraps google-genai client.caches API with get_or_create pattern and TTL-based lifecycle"
    - "Review node caches curated_topics as system context on retry (revision_count > 0) so only the draft varies"
    - "Editorial node caches trend_context + enriched_contexts on retry (revision_count > 0) so only feedback/draft changes"
    - "Cache keys are scoped per pipeline run via thread_id to prevent cross-run contamination"
    - "Cache creation failures are fire-and-forget — pipeline continues without caching on error"
    - "TokenUsage model tracks cached_tokens from response.usage_metadata.cached_content_token_count"
    - "Cache TTL is 3600s (1 hour) — short enough to auto-expire after pipeline completes"
    - "Minimum token threshold (2048) checked before attempting cache creation"
  artifacts:
    - path: "src/editorial_ai/caching/__init__.py"
      provides: "Package exports for CacheManager and get_cache_manager"
    - path: "src/editorial_ai/caching/cache_manager.py"
      provides: "CacheManager class with get_or_create and token threshold check"
      contains: "class CacheManager"
    - path: "src/editorial_ai/observability/models.py"
      provides: "TokenUsage with cached_tokens field"
      contains: "cached_tokens"
    - path: "tests/test_cache_manager.py"
      provides: "Unit tests for CacheManager lifecycle, threshold, key scoping, error handling"
  key_links:
    - from: "src/editorial_ai/caching/cache_manager.py"
      to: "src/editorial_ai/services/curation_service.py"
      via: "Uses get_genai_client() for google-genai Client"
    - from: "src/editorial_ai/nodes/review.py"
      to: "src/editorial_ai/caching/cache_manager.py"
      via: "review_node creates cache for curated_topics on retry"
    - from: "src/editorial_ai/nodes/editorial.py"
      to: "src/editorial_ai/caching/cache_manager.py"
      via: "editorial_node creates cache for trend_context on retry"
---

<objective>
Implement Vertex AI context caching for the review and editorial nodes on retry paths, reducing token costs by up to 90% on cached content.

Purpose: When review fails and editorial regenerates, the same curated_topics and trend_context are re-sent to the LLM. Caching these static contexts avoids re-processing thousands of tokens on each retry.
Output: `src/editorial_ai/caching/` package + review and editorial nodes using caches on retries + cached token tracking in observability.
</objective>

<execution_context>
@/Users/kiyeol/.claude-pers/get-shit-done/workflows/execute-plan.md
@/Users/kiyeol/.claude-pers/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/13-pipeline-advanced/13-CONTEXT.md
@.planning/phases/13-pipeline-advanced/13-RESEARCH.md
@.planning/phases/13-pipeline-advanced/13-01-SUMMARY.md
@src/editorial_ai/services/review_service.py
@src/editorial_ai/services/editorial_service.py
@src/editorial_ai/nodes/review.py
@src/editorial_ai/nodes/editorial.py
@src/editorial_ai/services/curation_service.py
@src/editorial_ai/observability/models.py
@src/editorial_ai/observability/collector.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: CacheManager + observability extension</name>
  <files>
    src/editorial_ai/caching/__init__.py
    src/editorial_ai/caching/cache_manager.py
    src/editorial_ai/observability/models.py
    src/editorial_ai/observability/collector.py
    tests/test_cache_manager.py
  </files>
  <action>
**Step 1: Create `src/editorial_ai/caching/cache_manager.py`:**

```python
"""Context caching manager for Gemini API.

Wraps google-genai client.caches API with:
- get_or_create pattern (reuse existing cache within a pipeline run)
- Minimum token threshold check (2048 tokens)
- TTL-based lifecycle (3600s default)
- Fire-and-forget error handling (never breaks pipeline)
"""

import logging
from google import genai
from google.genai import types

logger = logging.getLogger(__name__)

# Minimum tokens for cache to be worthwhile (below this, implicit caching handles it)
MIN_CACHE_TOKENS = 2048
# Rough chars-to-tokens ratio for threshold estimation (conservative)
CHARS_PER_TOKEN_ESTIMATE = 4


class CacheManager:
    """Manages explicit context caches for pipeline LLM calls."""

    def __init__(self, client: genai.Client) -> None:
        self._client = client
        self._active_caches: dict[str, str] = {}  # cache_key -> cache.name

    async def get_or_create(
        self,
        cache_key: str,
        model: str,
        contents: list[types.Content] | str,
        *,
        system_instruction: str | None = None,
        ttl: str = "3600s",
    ) -> str | None:
        """Get existing cache or create new one. Returns cache name or None.

        Returns None if:
        - Content is below minimum token threshold
        - Cache creation fails (fire-and-forget)
        - Cache already expired and re-creation fails
        """
        try:
            # Estimate token count from content length
            content_chars = self._estimate_chars(contents)
            if content_chars < MIN_CACHE_TOKENS * CHARS_PER_TOKEN_ESTIMATE:
                logger.debug(
                    "Content below cache threshold (%d chars < %d min), skipping cache for key=%s",
                    content_chars, MIN_CACHE_TOKENS * CHARS_PER_TOKEN_ESTIMATE, cache_key
                )
                return None

            # Check existing cache
            if cache_key in self._active_caches:
                try:
                    # Verify cache still exists (may have expired)
                    self._client.caches.get(name=self._active_caches[cache_key])
                    logger.debug("Reusing existing cache for key=%s", cache_key)
                    return self._active_caches[cache_key]
                except Exception:
                    logger.debug("Cached entry expired for key=%s, recreating", cache_key)
                    del self._active_caches[cache_key]

            # Normalize contents to list[Content]
            if isinstance(contents, str):
                content_list = [types.Content(
                    role="user",
                    parts=[types.Part.from_text(text=contents)]
                )]
            else:
                content_list = contents

            # Create cache
            config = types.CreateCachedContentConfig(
                contents=content_list,
                display_name=cache_key,
                ttl=ttl,
            )
            if system_instruction:
                config.system_instruction = system_instruction

            cache = await self._client.aio.caches.create(
                model=model,
                config=config,
            )
            self._active_caches[cache_key] = cache.name
            logger.info("Created cache for key=%s, name=%s", cache_key, cache.name)
            return cache.name

        except Exception:
            logger.warning("Cache creation failed for key=%s, proceeding without cache", cache_key, exc_info=True)
            return None

    def _estimate_chars(self, contents: list[types.Content] | str) -> int:
        """Rough character count estimation for threshold check."""
        if isinstance(contents, str):
            return len(contents)
        total = 0
        for content in contents:
            if hasattr(content, 'parts') and content.parts:
                for part in content.parts:
                    if hasattr(part, 'text') and part.text:
                        total += len(part.text)
        return total

    def clear(self) -> None:
        """Clear active cache references (does not delete server-side caches — TTL handles that)."""
        self._active_caches.clear()
```

Add a module-level factory:
```python
_manager_instance: CacheManager | None = None

def get_cache_manager(client: genai.Client | None = None) -> CacheManager:
    """Get or create singleton CacheManager."""
    global _manager_instance
    if _manager_instance is None:
        if client is None:
            from editorial_ai.services.curation_service import get_genai_client
            client = get_genai_client()
        _manager_instance = CacheManager(client)
    return _manager_instance
```

**Step 2: Create `src/editorial_ai/caching/__init__.py`:**

Export: `CacheManager`, `get_cache_manager`.

**Step 3: Extend `TokenUsage` in `observability/models.py`:**

Add `cached_tokens: int = 0` field to `TokenUsage`.

**Step 4: Extend `record_token_usage` in `observability/collector.py`:**

Add `cached_tokens: int = 0` parameter. Pass to `TokenUsage(... cached_tokens=cached_tokens)`.

**Step 5: Write `tests/test_cache_manager.py`:**

Tests (mock the google-genai Client):
1. `test_get_or_create_below_threshold` — short content returns None
2. `test_get_or_create_creates_cache` — sufficient content creates cache and returns name
3. `test_get_or_create_reuses_existing` — second call with same key returns cached name without creating
4. `test_get_or_create_recreates_expired` — if cache.get fails, creates new one
5. `test_get_or_create_fire_and_forget` — creation exception returns None (doesn't raise)
6. `test_estimate_chars_string` — string input returns len
7. `test_estimate_chars_content_list` — Content list sums part text lengths
8. `test_clear_resets_active` — clear empties active caches

Use pytest + unittest.mock. Mock `client.caches.get`, `client.caches.create`, `client.aio.caches.create`.
  </action>
  <verify>
1. `python -m pytest tests/test_cache_manager.py -v` — all tests pass
2. `python -c "from editorial_ai.caching import CacheManager, get_cache_manager; print('OK')"` — imports work
3. `python -c "from editorial_ai.observability.models import TokenUsage; t = TokenUsage(cached_tokens=100); print(t.cached_tokens)"` — prints 100
  </verify>
  <done>CacheManager with get_or_create, threshold check, fire-and-forget safety. TokenUsage tracks cached_tokens.</done>
</task>

<task type="auto">
  <name>Task 2: Wire caching into review and editorial retry paths</name>
  <files>
    src/editorial_ai/services/review_service.py
    src/editorial_ai/services/editorial_service.py
    src/editorial_ai/nodes/review.py
    src/editorial_ai/nodes/editorial.py
  </files>
  <action>
**Step 1: Update `ReviewService.evaluate_with_llm()` to accept and use cache:**

Add `cache_name: str | None = None` parameter:

```python
@retry_on_api_error
async def evaluate_with_llm(
    self, draft_json: str, curated_topics_json: str,
    *, rubric_config=None,
    revision_count: int = 0,
    cache_name: str | None = None,
) -> list[CriterionResult]:
```

When generating content config, if `cache_name` is provided:
```python
config = types.GenerateContentConfig(
    response_mime_type="application/json",
    response_schema=ReviewResult,
    temperature=0.0,
)
if cache_name:
    config.cached_content = cache_name
```

When `cache_name` is used, the `contents` parameter should contain ONLY the draft (the variable part). The curated_topics are in the cache. So modify the prompt construction:
- If `cache_name`: `prompt = build_review_prompt(draft_json, "", rubric_config=rubric_config)` — empty curated_topics since they're cached
- Actually, simpler approach: when cache_name is set, the curated_topics are in the cache as context. The prompt just needs the draft and evaluation instructions. Build a shorter prompt that references "the curated data provided in context" without including it inline.

**Better approach — keep it simple:** Don't split the prompt. Instead, cache the curated_topics as system_instruction in the cache, and send the full prompt (with draft) as the request content. The cache reduces cost on the curated_topics portion.

Implementation:
```python
if cache_name:
    # curated_topics already cached — send only draft + evaluation prompt
    config.cached_content = cache_name
```

Update `record_token_usage` call to extract `cached_content_token_count`:
```python
if hasattr(response, "usage_metadata") and response.usage_metadata:
    record_token_usage(
        prompt_tokens=getattr(response.usage_metadata, "prompt_token_count", 0) or 0,
        completion_tokens=getattr(response.usage_metadata, "candidates_token_count", 0) or 0,
        total_tokens=getattr(response.usage_metadata, "total_token_count", 0) or 0,
        cached_tokens=getattr(response.usage_metadata, "cached_content_token_count", 0) or 0,
        model_name=self.model,
        routing_reason=...,  # from router if already wired
    )
```

Update `evaluate()` to accept and pass through `cache_name`.

**Step 2: Update `review_node` to create cache on retries:**

In `nodes/review.py`:
```python
from editorial_ai.caching import get_cache_manager
```

Before calling `service.evaluate()`:
```python
revision_count = state.get("revision_count", 0)
cache_name = None

# Cache curated_topics on retry — same topics, different draft
if revision_count > 0 and curated_topics:
    import json
    topics_json = json.dumps(curated_topics, ensure_ascii=False)
    thread_id = state.get("thread_id") or "unknown"
    cache_key = f"review-topics-{thread_id}"

    try:
        from editorial_ai.routing import get_model_router
        model = get_model_router().resolve("review", revision_count=revision_count).model
        cache_mgr = get_cache_manager()
        cache_name = await cache_mgr.get_or_create(
            cache_key=cache_key,
            model=model,
            contents=topics_json,
            system_instruction="다음은 큐레이션된 토픽 데이터입니다. 에디토리얼 초안을 이 데이터와 대조하여 평가하세요.",
        )
    except Exception:
        logger.warning("Failed to create review cache, proceeding without", exc_info=True)
```

Pass `cache_name=cache_name` to `service.evaluate()`.

**Step 3: Update `EditorialService.generate_content()` to accept and use cache:**

Add `cache_name: str | None = None` parameter. When cache is set:
```python
config = types.GenerateContentConfig(
    response_mime_type="application/json",
    response_schema=EditorialContent,
    temperature=0.7,
)
if cache_name:
    config.cached_content = cache_name
```

Update `create_editorial()` to accept and pass through `cache_name`.

Extract `cached_content_token_count` in the `record_token_usage` call.

**Step 4: Update `editorial_node` to create cache on retries:**

In `nodes/editorial.py`:
```python
from editorial_ai.caching import get_cache_manager
```

Before calling `service.create_editorial()`:
```python
revision_count = state.get("revision_count", 0)
cache_name = None

# Cache trend_context + enriched on retry — same context, different feedback/draft
if revision_count > 0 and trend_context:
    thread_id = state.get("thread_id") or "unknown"
    cache_key = f"editorial-context-{thread_id}"

    try:
        from editorial_ai.routing import get_model_router
        model = get_model_router().resolve("editorial_content", revision_count=revision_count).model
        cache_mgr = get_cache_manager()
        cache_name = await cache_mgr.get_or_create(
            cache_key=cache_key,
            model=model,
            contents=trend_context,
            system_instruction="다음은 에디토리얼 작성을 위한 트렌드 컨텍스트 데이터입니다.",
        )
    except Exception:
        logger.warning("Failed to create editorial cache, proceeding without", exc_info=True)
```

Pass `cache_name=cache_name` and `revision_count=revision_count` to `service.create_editorial()`.

**Step 5: Update all record_token_usage calls in editorial_service.py and review_service.py** to extract cached_content_token_count:

After every `response = await self.client.aio.models.generate_content(...)`:
```python
cached_tokens=getattr(response.usage_metadata, "cached_content_token_count", 0) or 0,
```
  </action>
  <verify>
1. `python -c "from editorial_ai.graph import build_graph; print('OK')"` — graph compiles
2. `python -m pytest tests/test_review_service.py tests/test_review_node.py tests/test_editorial_node.py -v` — existing tests still pass (cache_name defaults to None)
3. `grep -r "cache_name" src/editorial_ai/nodes/review.py src/editorial_ai/nodes/editorial.py` — shows cache wiring in both nodes
4. `grep -r "cached_tokens" src/editorial_ai/services/` — shows cached token tracking
5. `python -m pytest tests/ -x --timeout=30` — all tests pass
  </verify>
  <done>Review and editorial nodes create caches on retry. Services accept cache_name and pass to Gemini API. Cached tokens tracked in observability. Fire-and-forget — cache failures don't break pipeline.</done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_cache_manager.py -v` — cache manager tests pass
2. `python -c "from editorial_ai.graph import build_graph; print('OK')"` — no import errors
3. `grep -r "get_cache_manager" src/editorial_ai/nodes/` — shows usage in review.py and editorial.py
4. `grep "cached_tokens" src/editorial_ai/observability/models.py` — TokenUsage has cached_tokens field
5. `python -m pytest tests/ -x --timeout=30` — all tests pass
6. Code review: cache creation only happens when `revision_count > 0` (retry path)
</verification>

<success_criteria>
- CacheManager get_or_create works with threshold check and TTL
- Review node caches curated_topics on retry
- Editorial node caches trend_context on retry
- Cache failures are fire-and-forget (never break pipeline)
- cached_tokens tracked in TokenUsage for observability
- All existing tests pass (cache_name defaults to None)
- Cache keys scoped by thread_id to prevent cross-run contamination
</success_criteria>

<output>
After completion, create `.planning/phases/13-pipeline-advanced/13-03-SUMMARY.md`
</output>
