---
phase: 13-pipeline-advanced
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/editorial_ai/rubrics/__init__.py
  - src/editorial_ai/rubrics/classifier.py
  - src/editorial_ai/rubrics/registry.py
  - src/editorial_ai/prompts/review.py
  - src/editorial_ai/services/review_service.py
  - src/editorial_ai/nodes/review.py
  - tests/test_rubrics.py
autonomous: true

must_haves:
  truths:
    - "ContentType enum defines at least fashion_magazine, tech_blog, and default types"
    - "classify_content_type maps seed keyword to ContentType via keyword-domain matching"
    - "RubricConfig holds content-type-specific evaluation criteria with weights and extra prompt instructions"
    - "RUBRIC_REGISTRY maps each ContentType to its RubricConfig"
    - "build_review_prompt accepts optional rubric_config parameter and injects type-specific criteria/instructions"
    - "ReviewService.evaluate passes content_type-derived rubric to build_review_prompt"
    - "review_node classifies content type from state and passes it to ReviewService"
    - "Default rubric (fashion_magazine) is used when classification is uncertain — this pipeline is fashion-first"
  artifacts:
    - path: "src/editorial_ai/rubrics/__init__.py"
      provides: "Package exports for ContentType, RubricConfig, classify_content_type, get_rubric"
    - path: "src/editorial_ai/rubrics/classifier.py"
      provides: "classify_content_type function with keyword-domain mapping"
      contains: "def classify_content_type"
    - path: "src/editorial_ai/rubrics/registry.py"
      provides: "ContentType enum, RubricConfig dataclass, RUBRIC_REGISTRY dict, get_rubric helper"
      contains: "class ContentType"
    - path: "src/editorial_ai/prompts/review.py"
      provides: "build_review_prompt with optional rubric_config parameter"
      contains: "rubric_config"
    - path: "tests/test_rubrics.py"
      provides: "Tests for classifier, registry, and prompt integration"
  key_links:
    - from: "src/editorial_ai/rubrics/classifier.py"
      to: "src/editorial_ai/rubrics/registry.py"
      via: "classify_content_type returns ContentType enum used to look up RubricConfig"
    - from: "src/editorial_ai/nodes/review.py"
      to: "src/editorial_ai/rubrics/classifier.py"
      via: "review_node calls classify_content_type with seed keyword"
    - from: "src/editorial_ai/prompts/review.py"
      to: "src/editorial_ai/rubrics/registry.py"
      via: "build_review_prompt uses RubricConfig to add type-specific criteria"
---

<objective>
Build an adaptive rubric system that classifies content type from the seed keyword and adjusts review evaluation criteria accordingly.

Purpose: Fashion magazine content needs visual appeal and trend relevance evaluation, while tech blog content needs technical depth and fact accuracy emphasis. A one-size-fits-all rubric under-evaluates domain-specific quality.
Output: `src/editorial_ai/rubrics/` package with classifier + registry, updated review prompt and service, integrated into review node.
</objective>

<execution_context>
@/Users/kiyeol/.claude-pers/get-shit-done/workflows/execute-plan.md
@/Users/kiyeol/.claude-pers/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/13-pipeline-advanced/13-CONTEXT.md
@.planning/phases/13-pipeline-advanced/13-RESEARCH.md
@src/editorial_ai/prompts/review.py
@src/editorial_ai/services/review_service.py
@src/editorial_ai/nodes/review.py
@src/editorial_ai/state.py
@src/editorial_ai/models/review.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rubric registry + content classifier</name>
  <files>
    src/editorial_ai/rubrics/__init__.py
    src/editorial_ai/rubrics/registry.py
    src/editorial_ai/rubrics/classifier.py
  </files>
  <action>
**Step 1: Create `src/editorial_ai/rubrics/registry.py`:**

```python
"""Adaptive rubric registry — content-type-specific evaluation criteria.

Maps ContentType to RubricConfig with weighted criteria and prompt additions
for the review LLM-as-a-Judge evaluation.
"""

from dataclasses import dataclass, field
from enum import Enum


class ContentType(str, Enum):
    FASHION_MAGAZINE = "fashion_magazine"
    TECH_BLOG = "tech_blog"
    LIFESTYLE = "lifestyle"
    DEFAULT = "default"


@dataclass
class RubricCriterion:
    """A single evaluation criterion with weight."""
    name: str
    weight: float  # 0.0 to 1.5 — higher = stricter scoring
    description: str  # Injected into review prompt


@dataclass
class RubricConfig:
    """Content-type-specific evaluation configuration."""
    content_type: ContentType
    criteria: list[RubricCriterion]
    prompt_additions: str  # Extra review instructions for this content type
```

Define `RUBRIC_REGISTRY: dict[ContentType, RubricConfig]` with these entries:

**FASHION_MAGAZINE:**
- hallucination (weight=1.0): Standard hallucination detection
- fact_accuracy (weight=1.0): Brand/celeb name accuracy
- content_completeness (weight=1.0): Structural requirements
- visual_appeal (weight=0.8): "시각적 표현력 — 패션 이미지와 스타일링 묘사가 생생하고 매거진에 적합한지 평가. 추상적이거나 밋밋한 표현은 감점."
- trend_relevance (weight=0.9): "트렌드 반영도 — 최신 패션 트렌드, 시즌 키워드, 런웨이 레퍼런스가 정확하게 반영되었는지 평가."
- prompt_additions: "패션 에디토리얼로서의 매력과 트렌드 정확성을 중점 평가하세요. 시각적 묘사의 풍부함과 브랜드/셀럽 언급의 자연스러움에 주목하세요."

**TECH_BLOG:**
- hallucination (weight=1.0): Standard
- fact_accuracy (weight=1.2): Higher weight — tech facts must be precise
- content_completeness (weight=1.0): Standard
- technical_depth (weight=0.9): "기술적 깊이 — 기술 개념의 설명이 충분한 깊이를 가지며, 표면적 나열이 아닌 실질적 분석을 포함하는지 평가."
- prompt_additions: "기술 블로그로서 용어의 정확성, 개념 설명의 깊이, 실용적 인사이트를 중점 평가하세요."

**LIFESTYLE:**
- hallucination (weight=1.0): Standard
- fact_accuracy (weight=0.8): Slightly relaxed for lifestyle
- content_completeness (weight=1.0): Standard
- engagement (weight=0.8): "독자 공감도 — 라이프스타일 콘텐츠로서 독자의 일상과 연결되는 공감 요소가 있는지, 실용적 팁이나 영감을 주는지 평가."
- prompt_additions: "라이프스타일 콘텐츠로서 독자 공감과 실용성을 중점 평가하세요."

**DEFAULT** = same as FASHION_MAGAZINE (this pipeline is fashion-first).

Add a helper function:
```python
def get_rubric(content_type: ContentType) -> RubricConfig:
    """Get rubric config for a content type, falling back to DEFAULT."""
    return RUBRIC_REGISTRY.get(content_type, RUBRIC_REGISTRY[ContentType.DEFAULT])
```

**Step 2: Create `src/editorial_ai/rubrics/classifier.py`:**

```python
"""Keyword-based content type classifier.

Simple rule-based classification from seed keyword and curated topic keywords.
No ML — just keyword domain matching. Default is FASHION_MAGAZINE.
"""
```

Define `KEYWORD_DOMAIN_MAP: dict[str, ContentType]` mapping keywords to content types:

Tech keywords: "AI", "tech", "developer", "coding", "programming", "software", "startup", "SaaS", "cloud", "API", "machine learning", "deep learning", "blockchain"

Fashion keywords: "fashion", "style", "trend", "runway", "couture", "streetwear", "vogue", "lookbook", "outfit", "styling"

Lifestyle keywords: "wellness", "travel", "home decor", "food", "fitness", "mindfulness", "interior", "recipe"

Function `classify_content_type(keyword: str, curated_topics: list[dict] | None = None) -> ContentType`:
1. Normalize keyword to lowercase
2. Check each domain keyword — if it appears as a substring in the seed keyword, return the mapped ContentType
3. If curated_topics provided, check `related_keywords` from each topic the same way
4. Default: return `ContentType.FASHION_MAGAZINE`

**Step 3: Create `src/editorial_ai/rubrics/__init__.py`:**

Export: `ContentType`, `RubricConfig`, `RubricCriterion`, `get_rubric`, `classify_content_type`, `RUBRIC_REGISTRY`.
  </action>
  <verify>
```bash
python -c "from editorial_ai.rubrics import ContentType, classify_content_type, get_rubric; ct = classify_content_type('AI fashion tech'); r = get_rubric(ct); print(f'{ct.value}: {len(r.criteria)} criteria')"
```
Expected: Shows content type with correct criteria count.

```bash
python -c "from editorial_ai.rubrics import classify_content_type, ContentType; assert classify_content_type('summer fashion trend') == ContentType.FASHION_MAGAZINE; assert classify_content_type('AI developer tools') == ContentType.TECH_BLOG; print('OK')"
```
  </verify>
  <done>Rubric registry and classifier are importable and correctly classify keywords to content types with appropriate criteria.</done>
</task>

<task type="auto">
  <name>Task 2: Integrate rubrics into review prompt + service + node</name>
  <files>
    src/editorial_ai/prompts/review.py
    src/editorial_ai/services/review_service.py
    src/editorial_ai/nodes/review.py
    tests/test_rubrics.py
  </files>
  <action>
**Step 1: Update `build_review_prompt` in `prompts/review.py`:**

Add optional parameter `rubric_config: RubricConfig | None = None`. Import RubricConfig at top (use TYPE_CHECKING guard to avoid circular imports if needed):

```python
from __future__ import annotations
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from editorial_ai.rubrics.registry import RubricConfig
```

When `rubric_config` is provided:
- Replace the hardcoded "## 평가 기준" section with dynamically generated criteria from `rubric_config.criteria`.
- For each criterion in the config, generate a numbered section like the existing format:
  ```
  ### {i}. {criterion.name} (가중치: {criterion.weight})
  - {criterion.description}
  - severity: critical, major, minor (기준에 따라 판단)
  ```
- Append the `rubric_config.prompt_additions` before the output format section.
- Update the output format to list the actual criterion names from the config (not hardcoded 3).

When `rubric_config` is None, keep the existing behavior unchanged (backward compatible).

**Step 2: Update `ReviewService` in `services/review_service.py`:**

- Add `content_type` parameter to `evaluate()` and `evaluate_with_llm()`:
  ```python
  async def evaluate_with_llm(
      self, draft_json: str, curated_topics_json: str,
      *, rubric_config: RubricConfig | None = None,
      revision_count: int = 0,
  ) -> list[CriterionResult]:
  ```
- Pass `rubric_config` to `build_review_prompt(draft_json, curated_topics_json, rubric_config=rubric_config)`.
- In `evaluate()`, accept and pass `rubric_config` through to `evaluate_with_llm()`.

**Step 3: Update `review_node` in `nodes/review.py`:**

- Import classifier: `from editorial_ai.rubrics import classify_content_type, get_rubric`
- Before calling `service.evaluate()`:
  1. Get seed keyword: `curation_input = state.get("curation_input") or {}; seed_keyword = curation_input.get("keyword", "")`
  2. Classify: `content_type = classify_content_type(seed_keyword, curated_topics)`
  3. Get rubric: `rubric_config = get_rubric(content_type)`
  4. Log: `logger.info("Review using %s rubric for keyword=%s", content_type.value, seed_keyword)`
- Pass `rubric_config=rubric_config` and `revision_count=state.get("revision_count", 0)` to `service.evaluate()`.

**Step 4: Write `tests/test_rubrics.py`:**

Tests:
1. `test_classify_fashion_keyword` — "summer fashion" -> FASHION_MAGAZINE
2. `test_classify_tech_keyword` — "AI tools for developers" -> TECH_BLOG
3. `test_classify_lifestyle_keyword` — "home decor trends" -> LIFESTYLE
4. `test_classify_default_fallback` — "random stuff" -> FASHION_MAGAZINE (default)
5. `test_classify_with_curated_topics` — keyword doesn't match but related_keywords do
6. `test_get_rubric_fashion` — returns config with 5 criteria including visual_appeal and trend_relevance
7. `test_get_rubric_tech` — returns config with 4 criteria including technical_depth
8. `test_get_rubric_unknown_fallback` — unknown type falls back to DEFAULT
9. `test_build_review_prompt_with_rubric` — prompt contains dynamic criteria from config
10. `test_build_review_prompt_backward_compat` — None rubric produces original prompt text

Use pytest. Mock nothing — these are pure functions and data structures.
  </action>
  <verify>
1. `python -m pytest tests/test_rubrics.py -v` — all tests pass
2. `python -c "from editorial_ai.prompts.review import build_review_prompt; from editorial_ai.rubrics import get_rubric, ContentType; p = build_review_prompt('{}', '[]', rubric_config=get_rubric(ContentType.TECH_BLOG)); assert 'technical_depth' in p.lower() or '기술적 깊이' in p; print('OK')"` — tech rubric injected into prompt
3. `python -c "from editorial_ai.graph import build_graph; print('OK')"` — graph still compiles
4. `python -m pytest tests/test_review_service.py tests/test_review_node.py -v` — existing tests still pass (backward compat)
  </verify>
  <done>Review prompt dynamically adapts to content type. Classifier maps keywords to rubric configs. Review node wired end-to-end. Existing tests pass with backward-compatible defaults.</done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_rubrics.py -v` — all rubric/classifier tests pass
2. `python -c "from editorial_ai.rubrics import classify_content_type, get_rubric; r = get_rubric(classify_content_type('fashion trend')); print(f'{r.content_type.value}: {[c.name for c in r.criteria]}')"` — shows fashion criteria with visual_appeal
3. `python -c "from editorial_ai.prompts.review import build_review_prompt; p = build_review_prompt('{}', '[]'); assert 'hallucination' in p; print('Backward compat OK')"` — original prompt works without rubric
4. `python -c "from editorial_ai.graph import build_graph; print('Graph OK')"` — no import errors
5. `python -m pytest tests/ -x --timeout=30` — all existing tests still pass
</verification>

<success_criteria>
- ContentType enum with fashion_magazine, tech_blog, lifestyle, default
- Keyword classifier correctly maps domains
- Each content type has distinct criteria with appropriate weights
- build_review_prompt dynamically generates criteria section from rubric
- Backward compatible — None rubric produces original prompt
- Review node classifies content type and passes rubric
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/13-pipeline-advanced/13-02-SUMMARY.md`
</output>
