---
phase: 01-graph-skeleton-llm-integration
plan: 03
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/editorial_ai/llm.py
  - tests/test_llm.py
autonomous: false

must_haves:
  truths:
    - "create_llm()으로 생성한 LLM 인스턴스에 프롬프트를 보내면 응답이 정상 반환된다"
    - "LANGSMITH_TRACING=true 설정 시 LLM 호출이 LangSmith 대시보드에 트레이스로 기록된다"
    - "GOOGLE_API_KEY만 설정하면 Developer API를, GOOGLE_GENAI_USE_VERTEXAI=true 설정하면 Vertex AI를 사용한다"
  artifacts:
    - path: "src/editorial_ai/llm.py"
      provides: "LLM 인스턴스 팩토리 함수"
      exports: ["create_llm"]
    - path: "tests/test_llm.py"
      provides: "LLM 팩토리 단위 테스트"
      contains: "test_create_llm"
  key_links:
    - from: "src/editorial_ai/llm.py"
      to: "src/editorial_ai/config.py"
      via: "settings import"
      pattern: "from editorial_ai\\.config import settings"
    - from: "src/editorial_ai/llm.py"
      to: "langchain_google_genai"
      via: "ChatGoogleGenerativeAI"
      pattern: "ChatGoogleGenerativeAI"
---

<objective>
Gemini LLM 호출 팩토리를 구축하고, 실제 API 호출을 검증하며, LangSmith 트레이싱 연동을 확인한다.

Purpose: 모든 에이전트 노드가 사용할 LLM 인터페이스를 확립하고, 관찰 가능성(observability)을 확보한다.
Output: create_llm() 팩토리 함수, LLM 호출 테스트, LangSmith 트레이싱 검증
</objective>

<execution_context>
@/Users/kiyeol/.claude-pers/get-shit-done/workflows/execute-plan.md
@/Users/kiyeol/.claude-pers/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-graph-skeleton-llm-integration/01-RESEARCH.md
@.planning/phases/01-graph-skeleton-llm-integration/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: LLM factory and unit tests</name>
  <files>src/editorial_ai/llm.py, tests/test_llm.py</files>
  <action>
    LLM 인스턴스 팩토리 함수와 단위 테스트를 구축한다.

    1. src/editorial_ai/llm.py 생성:
       ```python
       from langchain_google_genai import ChatGoogleGenerativeAI
       from editorial_ai.config import settings

       def create_llm(
           model: str | None = None,
           temperature: float = 0.7,
       ) -> ChatGoogleGenerativeAI:
           """LLM 인스턴스 팩토리.

           환경변수로 백엔드를 자동 결정:
           - GOOGLE_API_KEY만 설정 → Gemini Developer API
           - GOOGLE_GENAI_USE_VERTEXAI=true → Vertex AI

           Args:
               model: 모델명. None이면 settings.default_model 사용.
               temperature: 생성 온도. 기본값 0.7.

           Returns:
               ChatGoogleGenerativeAI 인스턴스.
           """
           return ChatGoogleGenerativeAI(
               model=model or settings.default_model,
               temperature=temperature,
               api_key=settings.google_api_key,
               project=settings.gcp_project_id,
               location=settings.gcp_location,
           )
       ```

       설계 결정:
       - model 파라미터 기본값을 settings에서 가져옴 → 노드별 모델 분리 용이
       - temperature 파라미터 노출 → Editorial(0.7 창의적) vs Review(0.3 엄격) 분리 가능
       - project가 None이면 Developer API, 값이 있으면 Vertex AI 자동 결정

    2. tests/test_llm.py 생성:
       ```python
       # test_create_llm_default: create_llm() 호출 시 ChatGoogleGenerativeAI 인스턴스 반환 확인
       # test_create_llm_custom_model: create_llm(model="gemini-2.5-pro") 시 모델명 반영 확인
       # test_create_llm_custom_temperature: temperature 파라미터 반영 확인
       ```

       이 테스트들은 API 호출 없이 인스턴스 생성만 확인 (GOOGLE_API_KEY 없이도 통과).
       ChatGoogleGenerativeAI 인스턴스의 model, temperature 속성을 검증.

    주의사항:
    - `ChatVertexAI`를 import하거나 사용하지 않는다
    - 단위 테스트에서는 실제 API 호출을 하지 않는다 (인스턴스 생성만)
    - settings 의존성을 통해 환경변수 기반 설정 전환이 가능함을 구조로 보여준다
  </action>
  <verify>
    - `uv run pytest tests/test_llm.py -v` 모든 테스트 통과
    - `uv run python -c "from editorial_ai.llm import create_llm; print('LLM factory OK')"` 성공
    - `uv run ruff check src/editorial_ai/llm.py tests/test_llm.py`
  </verify>
  <done>create_llm() 팩토리가 settings 기반으로 LLM 인스턴스를 생성하고, 단위 테스트가 API 호출 없이 인스턴스 생성을 검증한다</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    Gemini LLM 호출 팩토리 (create_llm)와 LangSmith 트레이싱 연동.
    실제 API 호출로 end-to-end 동작을 확인해야 한다.
  </what-built>
  <how-to-verify>
    사전 조건: .env 파일에 GOOGLE_API_KEY와 LANGSMITH_API_KEY가 설정되어 있어야 한다.
    (.env.example을 복사하여 실제 키를 입력)

    1. LLM 호출 테스트:
       ```bash
       uv run python -c "
       from editorial_ai.llm import create_llm
       llm = create_llm()
       response = llm.invoke('안녕하세요, 간단히 인사해주세요.')
       print(f'Response: {response.content[:200]}')
       print('LLM call: OK')
       "
       ```
       기대 결과: Gemini가 한국어로 인사 응답 반환

    2. LangSmith 트레이싱 확인:
       ```bash
       uv run python -c "
       import os
       os.environ['LANGSMITH_TRACING'] = 'true'
       from editorial_ai.llm import create_llm
       llm = create_llm()
       response = llm.invoke('LangSmith 트레이싱 테스트입니다.')
       print(f'Response: {response.content[:200]}')
       print('Check LangSmith dashboard for trace')
       "
       ```
       기대 결과: https://smith.langchain.com 대시보드에서 "editorial-ai-worker" 프로젝트에 트레이스 확인

    3. (선택) Structured output 테스트:
       ```bash
       uv run python -c "
       from pydantic import BaseModel
       from editorial_ai.llm import create_llm

       class Greeting(BaseModel):
           message: str
           language: str

       llm = create_llm()
       structured_llm = llm.with_structured_output(Greeting)
       result = structured_llm.invoke('Say hello in Korean')
       print(f'Message: {result.message}')
       print(f'Language: {result.language}')
       print('Structured output: OK')
       "
       ```
  </how-to-verify>
  <resume-signal>Type "approved" if LLM call returns response and LangSmith shows trace, or describe issues</resume-signal>
</task>

</tasks>

<verification>
```bash
# 1. 단위 테스트 (API 키 불필요)
uv run pytest tests/test_llm.py -v

# 2. 전체 테스트 스위트
uv run pytest tests/ -v

# 3. 전체 린트
uv run ruff check src/ tests/

# 4. (API 키 필요) E2E LLM 호출
uv run python -c "
from editorial_ai.llm import create_llm
llm = create_llm()
response = llm.invoke('Hello')
print(f'LLM responds: {response.content[:100]}')
"
```
</verification>

<success_criteria>
1. create_llm() 팩토리가 ChatGoogleGenerativeAI 인스턴스를 반환
2. 단위 테스트 통과 (API 키 없이)
3. 실제 LLM 호출 시 응답 반환 (checkpoint에서 확인)
4. LangSmith 대시보드에 트레이스 기록 (checkpoint에서 확인)
5. ruff check 통과
</success_criteria>

<output>
After completion, create `.planning/phases/01-graph-skeleton-llm-integration/01-03-SUMMARY.md`
</output>
