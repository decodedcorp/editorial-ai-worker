---
phase: 06-review-agent-feedback-loop
plan: 02
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - src/editorial_ai/prompts/editorial.py
  - src/editorial_ai/services/editorial_service.py
  - src/editorial_ai/nodes/editorial.py
  - tests/test_editorial_node.py
autonomous: true

must_haves:
  truths:
    - "Editorial prompt builder has a feedback-aware variant that injects review failures into the prompt"
    - "Feedback is placed BEFORE the main generation instructions for maximum LLM attention"
    - "Editorial node reads feedback_history from state and uses feedback-aware prompt on retry"
    - "First-time generation (no feedback) uses original prompt unchanged"
    - "Previous draft title is included in feedback context (not full draft)"
    - "EditorialService accepts optional feedback parameter for regeneration"
  artifacts:
    - path: "src/editorial_ai/prompts/editorial.py"
      provides: "build_content_generation_prompt_with_feedback() function"
      exports: ["build_content_generation_prompt_with_feedback"]
    - path: "src/editorial_ai/services/editorial_service.py"
      provides: "create_editorial with optional feedback_history and previous_draft parameters"
    - path: "src/editorial_ai/nodes/editorial.py"
      provides: "editorial_node reads feedback_history from state, passes to service on retry"
    - path: "tests/test_editorial_node.py"
      provides: "Tests for feedback injection behavior"
  key_links:
    - from: "src/editorial_ai/nodes/editorial.py"
      to: "src/editorial_ai/services/editorial_service.py"
      via: "create_editorial with feedback params"
      pattern: "feedback_history"
    - from: "src/editorial_ai/services/editorial_service.py"
      to: "src/editorial_ai/prompts/editorial.py"
      via: "build_content_generation_prompt_with_feedback import"
      pattern: "build_content_generation_prompt_with_feedback"
---

<objective>
Add feedback injection to the editorial pipeline so review failures improve regeneration.

Purpose: When the review agent fails a draft, the editorial node needs to receive structured feedback explaining what went wrong and incorporate it into the next generation prompt. This prevents the same mistakes from repeating and makes the retry loop productive.
Output: Feedback-aware prompt builder, updated EditorialService and editorial_node, tests verifying feedback flow.
</objective>

<execution_context>
@/Users/kiyeol/.claude-pers/get-shit-done/workflows/execute-plan.md
@/Users/kiyeol/.claude-pers/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-review-agent-feedback-loop/06-RESEARCH.md
@.planning/phases/06-review-agent-feedback-loop/06-CONTEXT.md
@.planning/phases/06-review-agent-feedback-loop/06-01-SUMMARY.md

@src/editorial_ai/prompts/editorial.py
@src/editorial_ai/services/editorial_service.py
@src/editorial_ai/nodes/editorial.py
@src/editorial_ai/state.py
@src/editorial_ai/models/review.py
@tests/test_editorial_node.py
@tests/test_editorial_service.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add feedback-aware prompt builder and update EditorialService</name>
  <files>
    src/editorial_ai/prompts/editorial.py
    src/editorial_ai/services/editorial_service.py
  </files>
  <action>
**Add to `src/editorial_ai/prompts/editorial.py`:**

Add a new function `build_content_generation_prompt_with_feedback` AFTER the existing `build_content_generation_prompt`. Do NOT modify the original function.

```python
def build_content_generation_prompt_with_feedback(
    keyword: str,
    trend_context: str,
    feedback_history: list[dict],
    previous_draft: dict | None = None,
) -> str:
    """Build editorial prompt with injected review feedback for retry iterations.

    Feedback is placed BEFORE the main generation instructions so the LLM
    prioritizes addressing prior failures.
    """
    feedback_section = "--- 이전 검수 피드백 (반드시 반영하세요) ---\n\n"

    for i, feedback in enumerate(feedback_history, 1):
        feedback_section += f"[시도 {i}]\n"
        for criterion in feedback.get("criteria", []):
            if not criterion.get("passed"):
                feedback_section += (
                    f"- {criterion['criterion']}: {criterion['reason']}\n"
                )
        suggestions = feedback.get("suggestions", [])
        if suggestions:
            feedback_section += f"개선 제안: {', '.join(suggestions)}\n"
        feedback_section += "\n"

    if previous_draft:
        prev_title = previous_draft.get("title", "N/A")
        feedback_section += f"이전 초안 제목: {prev_title}\n"
        feedback_section += "위 피드백을 반영하여 완전히 새로운 초안을 작성하세요.\n\n"

    # Feedback BEFORE main prompt for maximum LLM attention
    base_prompt = build_content_generation_prompt(keyword, trend_context)
    return feedback_section + base_prompt
```

Key design decisions:
- Feedback section is prepended (BEFORE), not appended -- LLMs pay more attention to early context
- Only failed criteria are included (passed criteria are noise)
- Previous draft is summarized by title only (not full content) to avoid reproducing same content
- Explicit instruction "완전히 새로운 초안을 작성하세요" to prevent copy-paste from failed draft

**Modify `src/editorial_ai/services/editorial_service.py`:**

Update `generate_content` and `create_editorial` to accept optional feedback parameters:

1. Add import at top:
```python
from editorial_ai.prompts.editorial import (
    build_content_generation_prompt,
    build_content_generation_prompt_with_feedback,  # NEW
    build_layout_image_prompt,
    build_layout_parsing_prompt,
    build_output_repair_prompt,
)
```

2. Update `generate_content` signature and prompt selection:
```python
async def generate_content(
    self,
    keyword: str,
    trend_context: str,
    *,
    feedback_history: list[dict] | None = None,
    previous_draft: dict | None = None,
) -> EditorialContent:
```

Inside `generate_content`, change the prompt building line from:
```python
prompt = build_content_generation_prompt(keyword, trend_context)
```
to:
```python
if feedback_history:
    prompt = build_content_generation_prompt_with_feedback(
        keyword, trend_context, feedback_history, previous_draft
    )
else:
    prompt = build_content_generation_prompt(keyword, trend_context)
```

3. Update `create_editorial` signature to pass through:
```python
async def create_editorial(
    self,
    keyword: str,
    trend_context: str,
    *,
    feedback_history: list[dict] | None = None,
    previous_draft: dict | None = None,
) -> MagazineLayout:
```

Pass `feedback_history` and `previous_draft` to `self.generate_content(...)`:
```python
content = await self.generate_content(
    keyword, trend_context,
    feedback_history=feedback_history,
    previous_draft=previous_draft,
)
```

These are keyword-only optional params, so no existing callers are broken.
  </action>
  <verify>
`uv run python -c "from editorial_ai.prompts.editorial import build_content_generation_prompt_with_feedback; p = build_content_generation_prompt_with_feedback('Y2K', 'trend', [{'criteria': [{'criterion': 'format', 'passed': False, 'reason': 'missing blocks'}]}]); assert '이전 검수 피드백' in p; print('prompt OK')"` succeeds.
`uv run pytest tests/test_editorial_service.py -v` -- existing tests still pass (no signature breakage).
  </verify>
  <done>
Feedback-aware prompt builder exists. EditorialService.generate_content and create_editorial accept optional feedback params. Existing behavior unchanged when no feedback provided.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update editorial node to inject feedback on retry</name>
  <files>
    src/editorial_ai/nodes/editorial.py
    tests/test_editorial_node.py
  </files>
  <action>
**Modify `src/editorial_ai/nodes/editorial.py`:**

Update `editorial_node` to read `feedback_history` and `current_draft` from state and pass them to the service on retry iterations.

Changes to the function body (do NOT change error handling or other logic):

1. After reading `curated_topics`, also read feedback:
```python
feedback_history = state.get("feedback_history") or []
previous_draft = state.get("current_draft") if feedback_history else None
```

2. In the try block, change the service call from:
```python
layout = await service.create_editorial(primary_keyword, trend_context)
```
to:
```python
layout = await service.create_editorial(
    primary_keyword,
    trend_context,
    feedback_history=feedback_history if feedback_history else None,
    previous_draft=previous_draft,
)
```

The full function maintains its existing structure. The only change is reading feedback from state and passing it through to the service.

**Update `tests/test_editorial_node.py`:**

Add tests verifying feedback injection behavior. Follow existing test patterns (mock EditorialService):

1. `test_editorial_node_passes_feedback_to_service` -- set state with `feedback_history=[{"criteria": [...], "suggestions": [...]}]` and a `current_draft` dict. Mock the service. Assert `create_editorial` was called with `feedback_history` and `previous_draft` kwargs.

2. `test_editorial_node_no_feedback_first_run` -- set state with empty `feedback_history=[]`. Assert `create_editorial` was called with `feedback_history=None` (or without feedback kwargs).

Mock strategy: Same as existing tests -- patch `EditorialService` or `get_genai_client` and mock `create_editorial` as AsyncMock returning a valid MagazineLayout.

Use the existing `_base_state` helper pattern from `test_enrich_node.py` for state construction.
  </action>
  <verify>
`uv run pytest tests/test_editorial_node.py -v` -- all tests pass (existing + new).
`uv run pytest tests/ -v` -- full test suite passes.
  </verify>
  <done>
Editorial node reads feedback_history from state and passes it to EditorialService on retry. First-run behavior unchanged. Tests verify feedback flows from state through node to service.
  </done>
</task>

</tasks>

<verification>
- `uv run pytest tests/test_editorial_node.py -v` -- all tests pass
- `uv run pytest tests/test_editorial_service.py -v` -- existing tests pass (no regression)
- `uv run pytest tests/ -v` -- full test suite passes
- `uv run python -c "from editorial_ai.prompts.editorial import build_content_generation_prompt_with_feedback"` -- imports OK
</verification>

<success_criteria>
- Feedback-aware prompt exists and prepends feedback BEFORE main instructions
- Editorial node reads feedback_history from state on retry
- First-time generation (empty feedback_history) uses original prompt unchanged
- Previous draft title (not full content) is included in feedback context
- EditorialService accepts optional feedback parameters without breaking existing callers
- All existing tests pass with no regressions
</success_criteria>

<output>
After completion, create `.planning/phases/06-review-agent-feedback-loop/06-02-SUMMARY.md`
</output>
