---
phase: 06-review-agent-feedback-loop
plan: 03
type: execute
wave: 3
depends_on: ["06-01", "06-02"]
files_modified:
  - src/editorial_ai/nodes/review.py
  - src/editorial_ai/graph.py
  - src/editorial_ai/nodes/stubs.py
  - tests/test_review_node.py
  - tests/test_graph.py
autonomous: true

must_haves:
  truths:
    - "review_node replaces stub_review in graph.py"
    - "review_node writes review_result, increments revision_count on failure, appends to feedback_history"
    - "review_node sets pipeline_status='awaiting_approval' on pass"
    - "review_node sets pipeline_status='failed' and appends escalation to error_log when revision_count >= 3 and review fails"
    - "review_node handles current_draft=None gracefully (no crash)"
    - "Existing route_after_review conditional edge still works correctly with real review_node output"
    - "Graph topology tests pass end-to-end with real review logic"
  artifacts:
    - path: "src/editorial_ai/nodes/review.py"
      provides: "review_node LangGraph node replacing stub_review"
      exports: ["review_node"]
      min_lines: 25
    - path: "src/editorial_ai/graph.py"
      provides: "Graph with real review_node imported and used"
      contains: "from editorial_ai.nodes.review import review_node"
    - path: "tests/test_review_node.py"
      provides: "Unit tests for review node state management"
    - path: "tests/test_graph.py"
      provides: "Updated graph tests compatible with real review node"
  key_links:
    - from: "src/editorial_ai/nodes/review.py"
      to: "src/editorial_ai/services/review_service.py"
      via: "ReviewService import"
      pattern: "from editorial_ai.services.review_service import ReviewService"
    - from: "src/editorial_ai/graph.py"
      to: "src/editorial_ai/nodes/review.py"
      via: "review_node import replaces stub_review in nodes dict"
      pattern: "from editorial_ai.nodes.review import review_node"
---

<objective>
Create the review LangGraph node, wire it into the graph replacing stub_review, and verify the full feedback loop works end-to-end.

Purpose: This is the final integration step -- the review node calls ReviewService, writes results to state, handles escalation on max retries, and the existing conditional edge routes based on the node's output. Replaces stub_review with real implementation.
Output: review_node function, updated graph.py, review node unit tests, updated graph tests.
</objective>

<execution_context>
@/Users/kiyeol/.claude-pers/get-shit-done/workflows/execute-plan.md
@/Users/kiyeol/.claude-pers/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-review-agent-feedback-loop/06-RESEARCH.md
@.planning/phases/06-review-agent-feedback-loop/06-CONTEXT.md
@.planning/phases/06-review-agent-feedback-loop/06-01-SUMMARY.md
@.planning/phases/06-review-agent-feedback-loop/06-02-SUMMARY.md

@src/editorial_ai/nodes/editorial.py
@src/editorial_ai/nodes/enrich.py
@src/editorial_ai/nodes/stubs.py
@src/editorial_ai/graph.py
@src/editorial_ai/state.py
@src/editorial_ai/services/review_service.py
@src/editorial_ai/models/review.py
@tests/test_graph.py
@tests/test_enrich_node.py
@tests/test_editorial_node.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create review node</name>
  <files>
    src/editorial_ai/nodes/review.py
  </files>
  <action>
**Create `src/editorial_ai/nodes/review.py`:**

Follow the exact pattern of `nodes/editorial.py` and `nodes/enrich.py` -- thin async wrapper:

```python
"""Review node for the editorial pipeline graph.

Thin wrapper around ReviewService: reads current_draft and curated_topics
from state, runs hybrid evaluation, writes review results back to state.
"""
from __future__ import annotations

import logging

from editorial_ai.services.curation_service import get_genai_client
from editorial_ai.services.review_service import ReviewService
from editorial_ai.state import EditorialPipelineState

logger = logging.getLogger(__name__)

MAX_REVISIONS = 3


async def review_node(state: EditorialPipelineState) -> dict:
    """LangGraph node: evaluate editorial draft quality.

    Reads current_draft and curated_topics from state, calls ReviewService
    for hybrid evaluation (Pydantic format + LLM semantic), writes review
    results back to state.

    On pass: sets pipeline_status = "awaiting_approval"
    On fail: increments revision_count, appends to feedback_history
    On escalation (revision_count >= MAX_REVISIONS and still failing):
        sets pipeline_status = "failed", appends to error_log
    """
    current_draft = state.get("current_draft")
    if not current_draft:
        revision_count = state.get("revision_count", 0) + 1
        return {
            "review_result": {"passed": False},
            "revision_count": revision_count,
            "feedback_history": [
                {"criteria": [], "summary": "No draft to review"}
            ],
            "error_log": ["Review skipped: no current_draft in state"],
        }

    curated_topics = state.get("curated_topics") or []

    try:
        service = ReviewService(get_genai_client())
        result = await service.evaluate(current_draft, curated_topics)
    except Exception as e:
        logger.exception("Review node failed")
        return {
            "review_result": {"passed": False},
            "revision_count": state.get("revision_count", 0) + 1,
            "feedback_history": [
                {"criteria": [], "summary": f"Review error: {e!s}"}
            ],
            "error_log": [f"Review failed: {type(e).__name__}: {e!s}"],
        }

    result_dict = result.model_dump()
    update: dict = {"review_result": result_dict}

    if result.passed:
        update["pipeline_status"] = "awaiting_approval"
    else:
        new_revision_count = state.get("revision_count", 0) + 1
        update["revision_count"] = new_revision_count
        update["feedback_history"] = [result_dict]  # appended via operator.add

        # Escalation: max retries reached and still failing
        if new_revision_count >= MAX_REVISIONS:
            update["pipeline_status"] = "failed"
            update["error_log"] = [
                f"Escalation: review failed after {new_revision_count} attempts. "
                f"Last failure: {result.summary}"
            ]

    return update
```

Key points:
- `MAX_REVISIONS = 3` matches `route_after_review` in graph.py which checks `revision_count >= 3`
- On pass: sets `pipeline_status = "awaiting_approval"` so route goes to admin_gate
- On fail (not escalation): increments revision_count, appends feedback_history (operator.add handles accumulation)
- On escalation: sets `pipeline_status = "failed"` and appends escalation message to error_log -- this is the clear terminal state mentioned in research pitfall 3
- `current_draft=None` handling: returns failure with feedback so the retry loop can still operate
- Exception handling: catches service failures gracefully, same pattern as editorial_node
- Does NOT change pipeline_status on non-escalation failure -- lets route_after_review handle routing
  </action>
  <verify>
`uv run python -c "from editorial_ai.nodes.review import review_node; print('import OK')"` succeeds.
  </verify>
  <done>
review_node exists as thin async wrapper around ReviewService. Handles pass, fail, escalation, no-draft, and exception cases.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire review node into graph and update all tests</name>
  <files>
    src/editorial_ai/graph.py
    src/editorial_ai/nodes/stubs.py
    tests/test_review_node.py
    tests/test_graph.py
  </files>
  <action>
**Modify `src/editorial_ai/graph.py`:**

1. Add import for real review_node:
```python
from editorial_ai.nodes.review import review_node
```

2. In the `nodes` dict inside `build_graph()`, change:
```python
"review": stub_review,
```
to:
```python
"review": review_node,
```

3. Keep `stub_review` in the stubs import with `# noqa: F401` for backward compat (tests use via node_overrides):
```python
from editorial_ai.nodes.stubs import (
    stub_admin_gate,
    stub_curation,  # noqa: F401
    stub_editorial,  # noqa: F401
    stub_enrich,  # noqa: F401
    stub_publish,
    stub_review,  # noqa: F401 â€” kept for backward compat (tests use via node_overrides)
    stub_source,
)
```

4. Update the module docstring to reflect that review is now a real node:
```
The graph follows: curation -> source -> editorial -> enrich -> review -> admin_gate -> publish
with conditional routing after review (retry/fail) and admin_gate (revision/reject).
Review node performs LLM-as-a-Judge evaluation (Phase 6).
```

**Update `src/editorial_ai/nodes/stubs.py`:**

Update the stub_review docstring to indicate it's now a backward-compat stub:
```python
def stub_review(state: EditorialPipelineState) -> dict:
    """Stub review node. Real implementation in nodes/review.py (Phase 6)."""
```

**Create `tests/test_review_node.py`:**

Follow the exact pattern of `test_enrich_node.py`:

```python
"""Tests for the review LangGraph node.

All tests mock ReviewService -- no real LLM calls.
Verifies state reads/writes, pass/fail routing, escalation, and error handling.
"""
from __future__ import annotations
from unittest.mock import AsyncMock, MagicMock, patch

from editorial_ai.models.layout import create_default_template
from editorial_ai.models.review import CriterionResult, ReviewResult
from editorial_ai.nodes.review import review_node
```

Helper:
```python
_PATCH_SERVICE = "editorial_ai.nodes.review.ReviewService"
_PATCH_CLIENT = "editorial_ai.nodes.review.get_genai_client"

def _base_state(**overrides: object) -> dict:
    """Minimal state for review node invocation."""
    state: dict = {
        "curation_input": {"keyword": "Y2K"},
        "curated_topics": [{"keyword": "Y2K", "trend_background": "Y2K revival"}],
        "enriched_contexts": [],
        "current_draft": create_default_template("Y2K", "Y2K Revival").model_dump(),
        "current_draft_id": None,
        "tool_calls_log": [],
        "review_result": None,
        "revision_count": 0,
        "feedback_history": [],
        "admin_decision": None,
        "admin_feedback": None,
        "pipeline_status": "reviewing",
        "error_log": [],
    }
    state.update(overrides)
    return state

def _passing_review() -> ReviewResult:
    return ReviewResult(
        passed=True,
        criteria=[
            CriterionResult(criterion="format", passed=True, reason="OK"),
            CriterionResult(criterion="hallucination", passed=True, reason="OK"),
            CriterionResult(criterion="fact_accuracy", passed=True, reason="OK"),
            CriterionResult(criterion="content_completeness", passed=True, reason="OK"),
        ],
        summary="All criteria passed",
    )

def _failing_review() -> ReviewResult:
    return ReviewResult(
        passed=False,
        criteria=[
            CriterionResult(criterion="format", passed=True, reason="OK"),
            CriterionResult(criterion="hallucination", passed=False, reason="Found fabricated brand", severity="critical"),
            CriterionResult(criterion="fact_accuracy", passed=True, reason="OK"),
            CriterionResult(criterion="content_completeness", passed=True, reason="OK"),
        ],
        summary="Hallucination detected",
        suggestions=["Remove fabricated brand references"],
    )
```

**Tests to write:**

1. **`TestReviewNodePass` class:**
   - `test_pass_sets_awaiting_approval` -- mock ReviewService.evaluate returning passing review. Assert `pipeline_status == "awaiting_approval"` and `review_result["passed"] is True`.
   - `test_pass_does_not_increment_revision_count` -- assert `revision_count` not in result (not incremented on pass).

2. **`TestReviewNodeFail` class:**
   - `test_fail_increments_revision_count` -- mock failing review with state revision_count=0. Assert result has `revision_count == 1`.
   - `test_fail_appends_feedback_history` -- assert result has `feedback_history` as a 1-item list containing the result dict.
   - `test_fail_does_not_set_pipeline_status` -- on non-escalation failure, pipeline_status should NOT be in the result (route_after_review handles routing).

3. **`TestReviewNodeEscalation` class:**
   - `test_escalation_sets_failed_status` -- mock failing review with state `revision_count=2` (will become 3). Assert `pipeline_status == "failed"` and `error_log` contains escalation message.
   - `test_escalation_still_appends_feedback` -- even on escalation, feedback_history is appended for audit trail.

4. **`TestReviewNodeNoDraft` class:**
   - `test_no_draft_returns_failure` -- state with `current_draft=None`. Assert `review_result["passed"] is False` and error_log mentions "no current_draft".

5. **`TestReviewNodeServiceError` class:**
   - `test_service_exception_returns_error` -- mock ReviewService.evaluate raising RuntimeError. Assert error_log contains the error, review_result is `{"passed": False}`.

Mock strategy: Patch `ReviewService` class at `editorial_ai.nodes.review.ReviewService`. The mock instance's `evaluate` method is an AsyncMock returning the appropriate ReviewResult.

```python
@patch(_PATCH_CLIENT, return_value=MagicMock())
@patch(_PATCH_SERVICE)
async def test_pass_sets_awaiting_approval(self, MockService, mock_client):
    instance = MockService.return_value
    instance.evaluate = AsyncMock(return_value=_passing_review())
    result = await review_node(_base_state())
    assert result["pipeline_status"] == "awaiting_approval"
```

**Update `tests/test_graph.py`:**

The existing graph tests use `stub_review` via `node_overrides`. Since graph.py now imports `review_node` as default, add `"review": stub_review` to ALL existing test node_overrides that don't already have it. This ensures existing graph topology tests continue to work without real LLM calls.

Check each test:
- `test_graph_happy_path` -- already uses stub_review via default? No, now review_node is default. Add `"review": stub_review` to overrides.
- Wait -- actually, existing tests like `test_graph_happy_path` already override curation, editorial, enrich. They DON'T override review because they relied on the default `stub_review`. Now that default is `review_node` (async, needs real LLM), these tests will break.

Fix: Import `stub_review` in test_graph.py and add it to all test node_overrides:

```python
from editorial_ai.nodes.stubs import stub_curation, stub_editorial, stub_enrich, stub_review
```

Update each test's `build_graph(node_overrides={...})` to include `"review": stub_review`.

Also update `test_graph_review_fail_then_pass` and `test_graph_max_retries` -- these already define custom mock_review functions, so they're fine. Just ensure the import exists.

New test (optional but valuable):
```python
def test_graph_has_real_review_node():
    """Default graph uses real review_node, not stub."""
    compiled = build_graph()
    # Verify review node is NOT the stub
    assert "review" in compiled.nodes
```
  </action>
  <verify>
`uv run pytest tests/test_review_node.py -v` -- all tests pass.
`uv run pytest tests/test_graph.py -v` -- all existing and new tests pass.
`uv run pytest tests/ -v` -- full test suite passes (no regressions).
  </verify>
  <done>
review_node replaces stub_review in graph. Graph tests updated with stub overrides for isolation. Review node tests cover: pass, fail, escalation, no-draft, service error. Full feedback loop verified via graph topology. Full test suite green.
  </done>
</task>

</tasks>

<verification>
- `uv run pytest tests/test_review_node.py -v` -- all review node tests pass
- `uv run pytest tests/test_graph.py -v` -- all graph tests pass (existing + updated)
- `uv run pytest tests/ -v` -- full test suite passes
- `uv run python -c "from editorial_ai.graph import build_graph; g = build_graph(); assert 'review' in g.nodes; print('review node in graph')"` -- confirms node exists
- Graph docstring reflects review is a real node (Phase 6)
- `stub_review` still importable for backward compat
</verification>

<success_criteria>
- review_node replaces stub_review as default in graph.py
- On pass: pipeline_status = "awaiting_approval", review_result has passed=True
- On fail: revision_count incremented, feedback_history appended
- On escalation (revision_count >= 3): pipeline_status = "failed", error_log has escalation message
- No-draft and service-error cases handled gracefully
- Existing route_after_review conditional edge works correctly with real node output
- All tests pass including existing graph topology tests
</success_criteria>

<output>
After completion, create `.planning/phases/06-review-agent-feedback-loop/06-03-SUMMARY.md`
</output>
