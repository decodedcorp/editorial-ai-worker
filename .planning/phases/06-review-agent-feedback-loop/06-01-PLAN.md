---
phase: 06-review-agent-feedback-loop
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/editorial_ai/models/review.py
  - src/editorial_ai/models/__init__.py
  - src/editorial_ai/prompts/review.py
  - src/editorial_ai/services/review_service.py
  - tests/test_review_service.py
autonomous: true

must_haves:
  truths:
    - "ReviewResult and CriterionResult Pydantic models exist with per-criterion pass/fail and severity"
    - "Format validation uses Pydantic MagazineLayout.model_validate() deterministically (no LLM)"
    - "LLM-as-a-Judge evaluates hallucination, fact_accuracy, content_completeness with temperature=0.0"
    - "Hybrid evaluation: Pydantic format check first, then LLM for semantic criteria"
    - "Overall pass requires all criteria passed (no critical failures)"
    - "Review prompt includes both draft JSON and curated_topics for fact-checking"
  artifacts:
    - path: "src/editorial_ai/models/review.py"
      provides: "ReviewResult and CriterionResult Pydantic models for structured evaluation output"
      exports: ["ReviewResult", "CriterionResult"]
      min_lines: 15
    - path: "src/editorial_ai/prompts/review.py"
      provides: "build_review_prompt for LLM-as-a-Judge evaluation"
      exports: ["build_review_prompt"]
      min_lines: 20
    - path: "src/editorial_ai/services/review_service.py"
      provides: "ReviewService with hybrid Pydantic+LLM evaluation"
      exports: ["ReviewService"]
      min_lines: 50
    - path: "src/editorial_ai/models/__init__.py"
      provides: "Updated exports including ReviewResult and CriterionResult"
    - path: "tests/test_review_service.py"
      provides: "Unit tests for review service and format validation"
  key_links:
    - from: "src/editorial_ai/services/review_service.py"
      to: "src/editorial_ai/models/review.py"
      via: "ReviewResult, CriterionResult imports"
      pattern: "from editorial_ai.models.review import"
    - from: "src/editorial_ai/services/review_service.py"
      to: "src/editorial_ai/models/layout.py"
      via: "MagazineLayout.model_validate for format check"
      pattern: "MagazineLayout.model_validate"
    - from: "src/editorial_ai/services/review_service.py"
      to: "src/editorial_ai/services/curation_service.py"
      via: "Reuse retry_on_api_error, get_genai_client, _strip_markdown_fences"
      pattern: "from editorial_ai.services.curation_service import"
---

<objective>
Create the review models, prompt, and service -- the core evaluation engine for the review agent.

Purpose: Implements the LLM-as-a-Judge evaluation with hybrid Pydantic+LLM approach. Format validation is deterministic via Pydantic schema check; hallucination, fact accuracy, and content completeness are evaluated by Gemini with structured output.
Output: ReviewResult/CriterionResult models, review prompt template, ReviewService with validate_format() + evaluate_with_llm() + evaluate() entry point, unit tests.
</objective>

<execution_context>
@/Users/kiyeol/.claude-pers/get-shit-done/workflows/execute-plan.md
@/Users/kiyeol/.claude-pers/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-review-agent-feedback-loop/06-RESEARCH.md
@.planning/phases/06-review-agent-feedback-loop/06-CONTEXT.md

@src/editorial_ai/models/layout.py
@src/editorial_ai/models/__init__.py
@src/editorial_ai/models/editorial.py
@src/editorial_ai/services/curation_service.py
@src/editorial_ai/services/editorial_service.py
@src/editorial_ai/prompts/editorial.py
@src/editorial_ai/prompts/curation.py
@src/editorial_ai/config.py
@src/editorial_ai/state.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create review models (ReviewResult, CriterionResult)</name>
  <files>
    src/editorial_ai/models/review.py
    src/editorial_ai/models/__init__.py
  </files>
  <action>
**Create `src/editorial_ai/models/review.py`:**

```python
from __future__ import annotations

from typing import Literal

from pydantic import BaseModel, Field


class CriterionResult(BaseModel):
    """Result for a single evaluation criterion."""

    criterion: Literal["hallucination", "format", "fact_accuracy", "content_completeness"]
    passed: bool
    reason: str  # Actionable explanation -- why it passed or failed
    severity: Literal["critical", "major", "minor"] = "major"


class ReviewResult(BaseModel):
    """Complete review evaluation result from LLM-as-a-Judge."""

    passed: bool  # Overall pass/fail
    criteria: list[CriterionResult]
    summary: str  # Brief overall assessment
    suggestions: list[str] = Field(default_factory=list)  # Improvement suggestions
```

Key design decisions:
- `criterion` is a Literal enum of exactly 4 values matching the CONTEXT.md decision
- `severity` has 3 levels: critical (must fix), major (should fix), minor (nice to fix)
- `suggestions` is a list for actionable improvement items
- `passed` on ReviewResult is the overall aggregate -- computed by the service, not the LLM

**Update `src/editorial_ai/models/__init__.py`:**

Add imports and exports for the new models:
```python
from editorial_ai.models.review import CriterionResult, ReviewResult
```

Add `"CriterionResult"` and `"ReviewResult"` to the `__all__` list in alphabetical order.
  </action>
  <verify>
`uv run python -c "from editorial_ai.models import ReviewResult, CriterionResult; r = CriterionResult(criterion='format', passed=True, reason='ok'); print(r.model_dump())"` prints valid dict.
  </verify>
  <done>
ReviewResult and CriterionResult models exist and are importable from editorial_ai.models. __init__.py exports updated.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create review prompt and ReviewService</name>
  <files>
    src/editorial_ai/prompts/review.py
    src/editorial_ai/services/review_service.py
  </files>
  <action>
**Create `src/editorial_ai/prompts/review.py`:**

A single prompt builder function following the pattern in `prompts/curation.py` and `prompts/editorial.py`:

```python
def build_review_prompt(draft_json: str, curated_topics_json: str) -> str:
```

The prompt must:
- Role: 패션 매거진 편집장 reviewing a draft
- Include the full draft Layout JSON for evaluation
- Include curated_topics_json as fact-checking ground truth
- Define 3 evaluation criteria (hallucination, fact_accuracy, content_completeness) -- format is handled by Pydantic, NOT by the LLM
- For hallucination: "큐레이션 데이터에 없는 허위 정보, 존재하지 않는 브랜드/셀럽/이벤트" -- be explicit that any info NOT in curated data is hallucination
- For fact_accuracy: "브랜드명, 셀럽명, 트렌드 설명이 큐레이션 데이터와 일치하는지"
- For content_completeness: "셀럽/인플루언서 참조 1개 이상, 상품/브랜드 참조 1개 이상, 본문 2개 이상 단락, 해시태그 포함"
- Each criterion must output: criterion name, passed bool, reason (actionable), severity
- End with "반드시 유효한 JSON만 출력하세요."
- Language: Korean for domain instructions, keep JSON field names in English

**Create `src/editorial_ai/services/review_service.py`:**

Follow the exact service pattern from `editorial_service.py` and `curation_service.py`:

```python
import json
import logging
from google import genai
from google.genai import types
from pydantic import ValidationError

from editorial_ai.config import settings
from editorial_ai.models.layout import MagazineLayout
from editorial_ai.models.review import CriterionResult, ReviewResult
from editorial_ai.prompts.review import build_review_prompt
from editorial_ai.services.curation_service import (
    _strip_markdown_fences,
    get_genai_client,
    retry_on_api_error,
)

class ReviewService:
    def __init__(self, client: genai.Client, *, model: str | None = None):
        self.client = client
        self.model = model or settings.default_model

    def validate_format(self, draft: dict) -> CriterionResult:
        """Deterministic format validation via Pydantic MagazineLayout."""
        # 1. Try MagazineLayout.model_validate(draft)
        # 2. On ValidationError -> return CriterionResult(criterion="format", passed=False, severity="critical")
        # 3. On success, check structural requirements:
        #    - title is non-empty
        #    - body_text block exists
        #    If missing -> passed=False, severity="critical" with specific reason
        # 4. All good -> passed=True, reason="Schema valid, structure complete"

    @retry_on_api_error
    async def evaluate_with_llm(
        self, draft_json: str, curated_topics_json: str
    ) -> list[CriterionResult]:
        """LLM-as-a-Judge for hallucination, fact_accuracy, content_completeness."""
        # 1. Build prompt via build_review_prompt()
        # 2. Call Gemini with:
        #    - response_mime_type="application/json"
        #    - response_schema=ReviewResult (Gemini structured output)
        #    - temperature=0.0 (deterministic evaluation)
        # 3. Parse response via ReviewResult.model_validate_json(_strip_markdown_fences(raw_text))
        # 4. Return result.criteria (list of CriterionResult)
        # Note: The LLM returns all criteria including format, but we only use non-format ones

    async def evaluate(
        self, draft: dict, curated_topics: list[dict]
    ) -> ReviewResult:
        """Full evaluation entry point: format (Pydantic) + LLM (semantic)."""
        # 1. Run validate_format(draft) -> format_result
        # 2. Run evaluate_with_llm(draft_json, topics_json) -> llm_criteria
        # 3. Combine: [format_result] + [c for c in llm_criteria if c.criterion != "format"]
        # 4. Compute overall_passed: all criteria must pass (any failure = overall fail)
        # 5. Build suggestions from failed criteria reasons
        # 6. Return ReviewResult(passed=overall_passed, criteria=all_criteria, summary=..., suggestions=...)
```

Key implementation details:
- Use `settings.default_model` (gemini-2.5-flash) for evaluation -- same as other services
- `validate_format` is sync (no LLM needed), `evaluate_with_llm` is async
- `evaluate` orchestrates both and computes the overall result
- Reuse `retry_on_api_error`, `_strip_markdown_fences`, `get_genai_client` from curation_service
- Constructor accepts optional `model` override for future flexibility
  </action>
  <verify>
`uv run python -c "from editorial_ai.services.review_service import ReviewService; from editorial_ai.prompts.review import build_review_prompt; print('imports OK')"` succeeds.
  </verify>
  <done>
ReviewService implements hybrid evaluation: deterministic Pydantic format check + LLM-as-a-Judge for semantic criteria. Review prompt includes draft + curated_topics for fact checking.
  </done>
</task>

<task type="auto">
  <name>Task 3: Unit tests for ReviewService</name>
  <files>
    tests/test_review_service.py
  </files>
  <action>
Create `tests/test_review_service.py` following the test patterns in `test_editorial_service.py` and `test_enrich_service.py`.

**Test structure:**

```python
import pytest
from unittest.mock import AsyncMock, MagicMock, patch

from editorial_ai.models.layout import MagazineLayout, create_default_template
from editorial_ai.models.review import CriterionResult, ReviewResult
from editorial_ai.services.review_service import ReviewService
```

**Tests to write:**

1. **`TestValidateFormat` class (sync, no mocks needed):**
   - `test_valid_layout_passes` -- pass a valid MagazineLayout.model_dump() with body_text block, assert criterion="format", passed=True
   - `test_invalid_schema_fails` -- pass `{"bad": "data"}`, assert passed=False, severity="critical"
   - `test_missing_body_text_fails` -- pass a valid layout dict but remove body_text blocks, assert passed=False with reason mentioning body_text
   - `test_empty_title_fails` -- pass layout with title="" (or verify this is caught), assert passed=False

2. **`TestEvaluateWithLLM` class (mock Gemini client):**
   - `test_llm_evaluation_returns_criteria` -- mock client.aio.models.generate_content to return a valid ReviewResult JSON string. Assert returned list has 3 CriterionResult items (hallucination, fact_accuracy, content_completeness).
   - `test_llm_evaluation_uses_temperature_zero` -- verify the generate_content call used temperature=0.0

3. **`TestEvaluate` class (mock both validate_format and evaluate_with_llm):**
   - `test_all_pass_returns_passed` -- mock format=pass + LLM all pass, assert result.passed is True
   - `test_format_fail_returns_failed` -- mock format=fail + LLM all pass, assert result.passed is False
   - `test_llm_fail_returns_failed` -- mock format=pass + one LLM criterion fails, assert result.passed is False
   - `test_suggestions_contain_failed_reasons` -- mock one failure, assert suggestions list is non-empty

**Mock strategy for Gemini client:**

```python
def _mock_genai_client(response_text: str) -> MagicMock:
    """Build mock genai client that returns given text."""
    client = MagicMock()
    mock_response = MagicMock()
    mock_response.text = response_text
    client.aio.models.generate_content = AsyncMock(return_value=mock_response)
    return client
```

For the valid ReviewResult JSON response, construct a ReviewResult with 3 passing criteria and serialize with `.model_dump_json()`.

Use `create_default_template("test", "Test Title")` to create valid layout dicts for testing.
  </action>
  <verify>
`uv run pytest tests/test_review_service.py -v` -- all tests pass.
  </verify>
  <done>
ReviewService unit tests cover: format validation (valid, invalid schema, missing body_text, empty title), LLM evaluation (returns criteria, temperature check), full evaluate orchestration (all pass, format fail, LLM fail, suggestions). All pass.
  </done>
</task>

</tasks>

<verification>
- `uv run pytest tests/test_review_service.py -v` -- all tests pass
- `uv run python -c "from editorial_ai.models import ReviewResult, CriterionResult"` -- imports OK
- `uv run python -c "from editorial_ai.services.review_service import ReviewService"` -- imports OK
- `uv run python -c "from editorial_ai.prompts.review import build_review_prompt"` -- imports OK
- `uv run pytest tests/ -v` -- full test suite passes (no regressions)
</verification>

<success_criteria>
- ReviewResult and CriterionResult models have per-criterion pass/fail with severity levels
- Format validation is deterministic via Pydantic (no LLM call)
- LLM evaluation covers hallucination, fact_accuracy, content_completeness with temperature=0.0
- Review prompt includes both draft and curated_topics for ground-truth fact checking
- ReviewService.evaluate() computes overall pass from all criteria
- All tests pass including existing test suite
</success_criteria>

<output>
After completion, create `.planning/phases/06-review-agent-feedback-loop/06-01-SUMMARY.md`
</output>
