---
phase: 10-pipeline-observability-backend
plan: 03
type: execute
wave: 2
depends_on: ["10-01"]
files_modified:
  - src/editorial_ai/api/routes/logs.py
  - src/editorial_ai/api/app.py
  - src/editorial_ai/api/schemas.py
autonomous: true

must_haves:
  truths:
    - "GET /api/contents/{content_id}/logs returns node-level execution logs in chronological order with total summary"
    - "Response includes per-node timing, token usage, status, and optionally IO snapshots"
    - "?include_io=false query param excludes input_state and output_state from response"
    - "Empty logs (no pipeline run yet) return 200 with empty runs array and null summary"
    - "Content ID is resolved to thread_id via content_service lookup"
  artifacts:
    - path: "src/editorial_ai/api/routes/logs.py"
      provides: "GET /api/contents/{content_id}/logs endpoint"
      contains: "def get_content_logs"
    - path: "src/editorial_ai/api/app.py"
      provides: "Router registration for logs endpoint"
      contains: "logs.router"
    - path: "src/editorial_ai/api/schemas.py"
      provides: "LogsResponse, NodeRunLogResponse Pydantic schemas"
      contains: "class LogsResponse"
  key_links:
    - from: "src/editorial_ai/api/routes/logs.py"
      to: "src/editorial_ai/observability/storage.py"
      via: "read_node_logs(thread_id) to fetch JSONL data"
    - from: "src/editorial_ai/api/routes/logs.py"
      to: "src/editorial_ai/services/content_service.py"
      via: "get_content_by_id to resolve content_id -> thread_id"
    - from: "src/editorial_ai/api/app.py"
      to: "src/editorial_ai/api/routes/logs.py"
      via: "include_router registration"
---

<objective>
Create the API endpoint that serves pipeline observability logs for a given content. Includes per-node run details and an aggregated summary.

Purpose: Provides the data layer that Phase 12 (Observability Dashboard) will consume. Also useful for debugging pipeline runs immediately.
Output: GET /api/contents/{content_id}/logs endpoint returning structured log data.
</objective>

<execution_context>
@/Users/kiyeol/.claude-pers/get-shit-done/workflows/execute-plan.md
@/Users/kiyeol/.claude-pers/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/10-pipeline-observability-backend/10-CONTEXT.md
@.planning/phases/10-pipeline-observability-backend/10-RESEARCH.md
@.planning/phases/10-pipeline-observability-backend/10-01-SUMMARY.md
@src/editorial_ai/api/app.py
@src/editorial_ai/api/routes/admin.py
@src/editorial_ai/api/schemas.py
@src/editorial_ai/services/content_service.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Logs API response schemas</name>
  <files>src/editorial_ai/api/schemas.py</files>
  <action>
Add these Pydantic response models to `schemas.py`:

1. **TokenUsageResponse(BaseModel):**
   - `prompt_tokens: int`
   - `completion_tokens: int`
   - `total_tokens: int`
   - `model_name: str | None = None`

2. **NodeRunLogResponse(BaseModel):**
   - `node_name: str`
   - `status: str` (success/error/skipped)
   - `started_at: datetime`
   - `ended_at: datetime`
   - `duration_ms: float`
   - `token_usage: list[TokenUsageResponse]`
   - `total_prompt_tokens: int`
   - `total_completion_tokens: int`
   - `total_tokens: int`
   - `prompt_chars: int`
   - `error_type: str | None = None`
   - `error_message: str | None = None`
   - `input_state: dict | None = None` (excluded when include_io=false)
   - `output_state: dict | None = None` (excluded when include_io=false)

3. **PipelineRunSummaryResponse(BaseModel):**
   - `thread_id: str`
   - `node_count: int`
   - `total_duration_ms: float`
   - `total_prompt_tokens: int`
   - `total_completion_tokens: int`
   - `total_tokens: int`
   - `status: str`
   - `started_at: datetime | None`
   - `ended_at: datetime | None`

4. **LogsResponse(BaseModel):**
   - `content_id: str`
   - `thread_id: str`
   - `runs: list[NodeRunLogResponse]`
   - `summary: PipelineRunSummaryResponse | None = None`

Import `datetime` if not already imported.
  </action>
  <verify>
`python -c "from editorial_ai.api.schemas import LogsResponse, NodeRunLogResponse, PipelineRunSummaryResponse; print('OK')"` succeeds.
  </verify>
  <done>API response schemas for logs endpoint are defined and importable.</done>
</task>

<task type="auto">
  <name>Task 2: Logs route + app registration</name>
  <files>
    src/editorial_ai/api/routes/logs.py
    src/editorial_ai/api/app.py
  </files>
  <action>
**Create `src/editorial_ai/api/routes/logs.py`:**

```python
router = APIRouter(dependencies=[Depends(verify_api_key)])

@router.get("/{content_id}/logs", response_model=LogsResponse)
async def get_content_logs(content_id: str, include_io: bool = True):
```

Implementation:
1. Look up content via `get_content_by_id(content_id)`. If not found, raise 404.
2. Extract `thread_id` from content dict.
3. Call `read_node_logs(thread_id)` from `editorial_ai.observability.storage`.
4. Convert each `NodeRunLog` to `NodeRunLogResponse`:
   - If `include_io=False`, set `input_state=None, output_state=None`
   - Otherwise include full IO data
5. Sort runs by `started_at` ascending (chronological).
6. If runs exist, build `PipelineRunSummaryResponse` using `PipelineRunSummary.from_logs()`.
7. Return `LogsResponse(content_id=content_id, thread_id=thread_id, runs=runs, summary=summary)`.

Edge cases:
- No logs yet (pipeline hasn't run or is just starting): Return `{"runs": [], "summary": null}` with 200.
- Partial logs (pipeline still running): Return whatever logs exist so far — caller can detect incomplete by checking node_count < 7.

**Register in `app.py`:**
- Import: `from editorial_ai.api.routes import logs`
- Add router: `app.include_router(logs.router, prefix="/api/contents", tags=["logs"])`
- Place BEFORE the existing admin router to avoid route conflicts, OR use the same prefix since admin's "/" and "/{id}" won't conflict with "/{id}/logs" path.

NOTE: The admin router is already at `/api/contents`. The logs route `/{content_id}/logs` can be added to the admin router directly, OR as a separate router at the same prefix. Prefer a separate router file for clean separation. Since FastAPI handles path matching by specificity, `/{content_id}/logs` won't conflict with `/{content_id}` (different path depth).
  </action>
  <verify>
1. `python -c "from editorial_ai.api.app import app; routes = [r.path for r in app.routes]; print([r for r in routes if 'logs' in r])"` — shows the logs route registered
2. Server starts without errors: `timeout 5 python -m uvicorn editorial_ai.api.app:app --port 18765 2>&1 | head -5` (may fail on missing env vars but import should succeed)
  </verify>
  <done>GET /api/contents/{content_id}/logs endpoint is registered, returns chronological node logs with summary, supports include_io query param, handles empty/partial states gracefully.</done>
</task>

</tasks>

<verification>
1. Schema imports work: `python -c "from editorial_ai.api.schemas import LogsResponse"`
2. Route registered: logs route appears in FastAPI app routes
3. Response structure: Empty logs return `{"runs": [], "summary": null}` with 200
4. include_io=false: IO fields are null in response when query param is false
5. Chronological ordering: Runs sorted by started_at ascending
</verification>

<success_criteria>
- GET /api/contents/{content_id}/logs endpoint returns structured observability data
- Response includes per-node metrics and aggregated summary
- include_io query param controls IO data inclusion
- 404 for unknown content_id, 200 with empty runs for content without logs
- Endpoint is authenticated via verify_api_key
</success_criteria>

<output>
After completion, create `.planning/phases/10-pipeline-observability-backend/10-03-SUMMARY.md`
</output>
