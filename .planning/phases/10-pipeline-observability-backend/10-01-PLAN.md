---
phase: 10-pipeline-observability-backend
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/editorial_ai/observability/__init__.py
  - src/editorial_ai/observability/models.py
  - src/editorial_ai/observability/collector.py
  - src/editorial_ai/observability/storage.py
  - data/logs/.gitkeep
autonomous: true

must_haves:
  truths:
    - "NodeRunLog Pydantic model can serialize node execution metrics including timing, tokens, status, model info, and IO snapshots"
    - "TokenCollector context var accumulates token usage from multiple LLM calls within a single node and resets cleanly between nodes"
    - "JSONL storage writes one log line per node execution to data/logs/{thread_id}.jsonl and reads back correctly"
    - "Storage read/write failures do not raise exceptions (fire-and-forget safety)"
  artifacts:
    - path: "src/editorial_ai/observability/__init__.py"
      provides: "Package exports"
    - path: "src/editorial_ai/observability/models.py"
      provides: "NodeRunLog and PipelineRunSummary Pydantic models"
      contains: "class NodeRunLog"
    - path: "src/editorial_ai/observability/collector.py"
      provides: "TokenCollector context var with record_token_usage and harvest_tokens functions"
      contains: "ContextVar"
    - path: "src/editorial_ai/observability/storage.py"
      provides: "append_node_log and read_node_logs functions for JSONL file I/O"
      contains: "jsonl"
    - path: "data/logs/.gitkeep"
      provides: "Log directory exists and is gitignored"
  key_links:
    - from: "src/editorial_ai/observability/collector.py"
      to: "src/editorial_ai/observability/models.py"
      via: "TokenCollector populates TokenUsage fields consumed by NodeRunLog"
    - from: "src/editorial_ai/observability/storage.py"
      to: "src/editorial_ai/observability/models.py"
      via: "storage serializes/deserializes NodeRunLog to/from JSONL"
---

<objective>
Create the observability data models, token collector context var, and JSONL file storage layer.

Purpose: Provides the foundational types and I/O for node-level pipeline instrumentation. Without this, the wrapper (Plan 02) and API (Plan 03) have nothing to work with.
Output: `src/editorial_ai/observability/` package with models, collector, and storage modules.
</objective>

<execution_context>
@/Users/kiyeol/.claude-pers/get-shit-done/workflows/execute-plan.md
@/Users/kiyeol/.claude-pers/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-pipeline-observability-backend/10-CONTEXT.md
@.planning/phases/10-pipeline-observability-backend/10-RESEARCH.md
@src/editorial_ai/state.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Observability Pydantic models</name>
  <files>
    src/editorial_ai/observability/__init__.py
    src/editorial_ai/observability/models.py
  </files>
  <action>
Create `src/editorial_ai/observability/` package.

In `models.py`, define these Pydantic v2 models:

1. **TokenUsage(BaseModel):**
   - `prompt_tokens: int = 0`
   - `completion_tokens: int = 0`
   - `total_tokens: int = 0`
   - `model_name: str | None = None`

2. **NodeRunLog(BaseModel):**
   - `thread_id: str`
   - `node_name: str`
   - `status: Literal["success", "error", "skipped"]`
   - `started_at: datetime` (UTC)
   - `ended_at: datetime` (UTC)
   - `duration_ms: float` (computed: `(ended_at - started_at).total_seconds() * 1000`)
   - `token_usage: list[TokenUsage]` (multiple LLM calls per node possible)
   - `total_prompt_tokens: int` (sum of all token_usage prompt_tokens)
   - `total_completion_tokens: int` (sum of all token_usage completion_tokens)
   - `total_tokens: int` (sum of all token_usage total_tokens)
   - `input_state: dict | None = None` (full state snapshot before node)
   - `output_state: dict | None = None` (state delta after node)
   - `error_type: str | None = None`
   - `error_message: str | None = None`
   - `error_traceback: str | None = None` (first 5 lines only)
   - `prompt_chars: int = 0` (total prompt character length across calls)

   Use `model_computed_fields` or `@computed_field` for duration_ms and token sums. Alternatively, use `model_validator(mode='before')` to compute them at construction time.

3. **PipelineRunSummary(BaseModel):**
   - `thread_id: str`
   - `node_count: int`
   - `total_duration_ms: float`
   - `total_prompt_tokens: int`
   - `total_completion_tokens: int`
   - `total_tokens: int`
   - `status: Literal["completed", "failed", "running"]`
   - `started_at: datetime | None`
   - `ended_at: datetime | None`

   Include a `@classmethod from_logs(cls, thread_id: str, logs: list[NodeRunLog]) -> PipelineRunSummary` that aggregates.

In `__init__.py`, export the key classes: `NodeRunLog`, `TokenUsage`, `PipelineRunSummary`.
  </action>
  <verify>
`python -c "from editorial_ai.observability.models import NodeRunLog, TokenUsage, PipelineRunSummary; print('OK')"` succeeds.
  </verify>
  <done>Three Pydantic models importable, with token aggregation and duration computation logic.</done>
</task>

<task type="auto">
  <name>Task 2: Token collector context var + JSONL storage</name>
  <files>
    src/editorial_ai/observability/collector.py
    src/editorial_ai/observability/storage.py
    data/logs/.gitkeep
  </files>
  <action>
**collector.py — Token accumulation via ContextVar:**

Create a `contextvars.ContextVar[list[TokenUsage]]` named `_token_usage_var` with default factory `list`.

Functions:
- `reset_token_collector() -> None`: Reset the context var to empty list. Called at start of each node.
- `record_token_usage(prompt_tokens: int, completion_tokens: int, total_tokens: int, model_name: str | None = None) -> None`: Append a `TokenUsage` to the current context var. Wrap in try/except — never raise. Log warning on failure.
- `harvest_tokens() -> list[TokenUsage]`: Return and clear the accumulated list. Called at end of each node by wrapper.

**storage.py — JSONL file I/O:**

Functions:
- `_log_dir() -> Path`: Return `Path("data/logs")`. Create dir if not exists (parents=True, exist_ok=True).
- `_log_path(thread_id: str) -> Path`: Return `_log_dir() / f"{thread_id}.jsonl"`.
- `append_node_log(log: NodeRunLog) -> None`: Append one JSON line to the thread's JSONL file. Use `log.model_dump_json()` + newline. Wrap entire function in try/except — log warning on failure, never raise (fire-and-forget).
- `read_node_logs(thread_id: str) -> list[NodeRunLog]`: Read all lines from thread's JSONL file, parse each as NodeRunLog. Return empty list if file doesn't exist or on any error. Never raise.

**data/logs/.gitkeep:** Create empty file. Add `data/logs/*.jsonl` to `.gitignore`.

Update `__init__.py` to also export `record_token_usage`, `reset_token_collector`, `harvest_tokens`, `append_node_log`, `read_node_logs`.
  </action>
  <verify>
`python -c "from editorial_ai.observability import record_token_usage, reset_token_collector, harvest_tokens, append_node_log, read_node_logs; print('OK')"` succeeds.
  </verify>
  <done>Token collector accumulates across async calls within a node. JSONL storage appends/reads logs. Both are fire-and-forget safe (no exceptions escape).</done>
</task>

</tasks>

<verification>
1. All imports work: `python -c "from editorial_ai.observability import NodeRunLog, TokenUsage, PipelineRunSummary, record_token_usage, harvest_tokens, append_node_log, read_node_logs"`
2. Round-trip test: Create a NodeRunLog, append_node_log it, read_node_logs it back, verify equality
3. TokenCollector isolation: reset, record 2 usages, harvest returns 2, harvest again returns 0
4. Fire-and-forget: storage write to invalid path logs warning but doesn't raise
</verification>

<success_criteria>
- observability package importable with all exports
- NodeRunLog captures timing, tokens, status, IO, errors
- TokenCollector accumulates per-node token usage via ContextVar
- JSONL append/read works end-to-end
- All operations are fire-and-forget safe
</success_criteria>

<output>
After completion, create `.planning/phases/10-pipeline-observability-backend/10-01-SUMMARY.md`
</output>
