---
phase: 10-pipeline-observability-backend
plan: 02
type: execute
wave: 2
depends_on: ["10-01"]
files_modified:
  - src/editorial_ai/observability/node_wrapper.py
  - src/editorial_ai/services/curation_service.py
  - src/editorial_ai/services/editorial_service.py
  - src/editorial_ai/services/review_service.py
  - src/editorial_ai/services/enrich_service.py
  - src/editorial_ai/graph.py
autonomous: true

must_haves:
  truths:
    - "node_wrapper decorator captures timing, state snapshots, token usage, and error info for each node execution"
    - "node_wrapper handles both sync and async node functions"
    - "record_token_usage is called after every generate_content call in service layer (8 call sites)"
    - "Observability collection failure does not interrupt pipeline execution"
    - "build_graph applies node_wrapper to all 7 nodes at graph build time"
  artifacts:
    - path: "src/editorial_ai/observability/node_wrapper.py"
      provides: "node_wrapper decorator function"
      contains: "def node_wrapper"
    - path: "src/editorial_ai/graph.py"
      provides: "Graph with wrapped nodes"
      contains: "node_wrapper"
  key_links:
    - from: "src/editorial_ai/observability/node_wrapper.py"
      to: "src/editorial_ai/observability/collector.py"
      via: "reset_token_collector at start, harvest_tokens at end"
    - from: "src/editorial_ai/observability/node_wrapper.py"
      to: "src/editorial_ai/observability/storage.py"
      via: "append_node_log after each node execution"
    - from: "src/editorial_ai/services/curation_service.py"
      to: "src/editorial_ai/observability/collector.py"
      via: "record_token_usage after generate_content calls"
    - from: "src/editorial_ai/graph.py"
      to: "src/editorial_ai/observability/node_wrapper.py"
      via: "wrapping all nodes in build_graph"
---

<objective>
Implement the node_wrapper decorator that instruments every pipeline node with timing, state capture, and token collection. Inject record_token_usage calls into all LLM service methods. Wire wrapper into build_graph.

Purpose: This is the core instrumentation — without it, no observability data is collected during pipeline runs.
Output: All 7 pipeline nodes automatically produce NodeRunLog entries when executed.
</objective>

<execution_context>
@/Users/kiyeol/.claude-pers/get-shit-done/workflows/execute-plan.md
@/Users/kiyeol/.claude-pers/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/10-pipeline-observability-backend/10-CONTEXT.md
@.planning/phases/10-pipeline-observability-backend/10-RESEARCH.md
@.planning/phases/10-pipeline-observability-backend/10-01-SUMMARY.md
@src/editorial_ai/graph.py
@src/editorial_ai/state.py
@src/editorial_ai/services/curation_service.py
@src/editorial_ai/services/editorial_service.py
@src/editorial_ai/services/review_service.py
@src/editorial_ai/services/enrich_service.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: node_wrapper decorator</name>
  <files>
    src/editorial_ai/observability/node_wrapper.py
    src/editorial_ai/observability/__init__.py
  </files>
  <action>
Create `node_wrapper.py` with a decorator factory:

```python
def node_wrapper(node_name: str):
    """Decorator that wraps a LangGraph node function with observability instrumentation."""
```

The wrapper must:

1. **Handle both sync and async:** Check `asyncio.iscoroutinefunction(fn)` to decide. Wrap sync in async if needed, or create both variants. LangGraph nodes in this project are all async, but be safe.

2. **Before node execution:**
   - `reset_token_collector()` — clear any leftover tokens
   - Snapshot `started_at = datetime.now(timezone.utc)`
   - Snapshot input state: `input_state = _safe_serialize(state)` (deep copy the state dict, catch serialization errors)

3. **Execute the node:** Call `fn(state)` (or `await fn(state)` for async). Catch all exceptions.

4. **After node execution (success):**
   - `ended_at = datetime.now(timezone.utc)`
   - `token_usage = harvest_tokens()`
   - `output_state = _safe_serialize(result)` (the returned state delta dict)
   - Create `NodeRunLog(thread_id=state.get("thread_id", "unknown"), node_name=node_name, status="success", ...)`
   - `append_node_log(log)`
   - Return original result unchanged

5. **After node execution (error):**
   - Still create NodeRunLog with `status="error"`, capture error_type, error_message, error_traceback (first 5 lines of traceback.format_exc())
   - `append_node_log(log)`
   - **Re-raise the exception** — observability must not swallow errors

6. **_safe_serialize(obj):** A helper that tries `json.loads(json.dumps(obj, default=str))` to get a JSON-safe dict. On failure, return `{"_serialization_error": str(error)}`. This handles Pydantic models, bytes, etc.

7. **Entire wrapper logic (except re-raise) in try/except:** If the wrapper itself fails (e.g., storage error), log warning and continue — don't break the pipeline.

Update `__init__.py` to export `node_wrapper`.
  </action>
  <verify>
`python -c "from editorial_ai.observability import node_wrapper; print('OK')"` succeeds.
  </verify>
  <done>node_wrapper decorator captures timing, state, tokens, errors for any LangGraph node function.</done>
</task>

<task type="auto">
  <name>Task 2: Inject record_token_usage into service layer + wire graph</name>
  <files>
    src/editorial_ai/services/curation_service.py
    src/editorial_ai/services/editorial_service.py
    src/editorial_ai/services/review_service.py
    src/editorial_ai/services/enrich_service.py
    src/editorial_ai/graph.py
  </files>
  <action>
**Service layer injection — add `record_token_usage` call after every `generate_content` call:**

The google-genai SDK's `GenerateContentResponse` has `usage_metadata` with `prompt_token_count`, `candidates_token_count`, `total_token_count`. After each `await self.client.aio.models.generate_content(...)` call that stores the response, add:

```python
from editorial_ai.observability import record_token_usage

# After: response = await self.client.aio.models.generate_content(...)
if hasattr(response, 'usage_metadata') and response.usage_metadata:
    record_token_usage(
        prompt_tokens=getattr(response.usage_metadata, 'prompt_token_count', 0) or 0,
        completion_tokens=getattr(response.usage_metadata, 'candidates_token_count', 0) or 0,
        total_tokens=getattr(response.usage_metadata, 'total_token_count', 0) or 0,
        model_name=getattr(self, 'model_name', None) or "gemini-2.5-flash",
    )
```

Inject at all 8 call sites identified in research:
1. `CurationService.research_trend()` — 1 call (line ~121)
2. `CurationService.expand_subtopics()` — 1 call (line ~139)
3. `CurationService.extract_topic()` — 1 call (line ~171)
4. `EditorialService.generate_content()` — 1 call (line ~124)
5. `EditorialService.generate_layout_image()` — 1 call (line ~173)
6. `EditorialService.parse_layout_image()` — 1 call (line ~233)
7. `EditorialService.repair_output()` — 1 call (line ~291)
8. `ReviewService.evaluate_with_llm()` — 1 call (line ~97)

For `enrich_service.py` (module-level functions, not class methods): same pattern but use a default model name string.

**IMPORTANT:** Use `getattr` with defaults throughout — if `usage_metadata` is None or fields don't exist, just skip (fire-and-forget).

**Graph wiring — modify `build_graph()` in `graph.py`:**

Import `node_wrapper` from `editorial_ai.observability`. In `build_graph()`, after the `nodes` dict is built (and after any overrides are applied), wrap each node:

```python
from editorial_ai.observability import node_wrapper

# After node_overrides are applied:
for name in list(nodes.keys()):
    nodes[name] = node_wrapper(name)(nodes[name])
```

This ensures ALL nodes (including overridden stubs in tests) get wrapped.
  </action>
  <verify>
1. `python -c "from editorial_ai.graph import build_graph; print('OK')"` — graph still compiles
2. `grep -r "record_token_usage" src/editorial_ai/services/` shows 8+ occurrences
3. `grep "node_wrapper" src/editorial_ai/graph.py` shows wrapper applied
  </verify>
  <done>All 8 LLM call sites record token usage. All 7 graph nodes are wrapped with observability. Graph still compiles without errors.</done>
</task>

</tasks>

<verification>
1. `python -c "from editorial_ai.graph import build_graph; g = build_graph(); print('Graph compiled OK')"` — no import/compilation errors
2. `grep -c "record_token_usage" src/editorial_ai/services/*.py` — shows injection across all 4 service files
3. `grep "node_wrapper" src/editorial_ai/graph.py` — wrapper is applied in build_graph
4. Manual trace: node_wrapper resets collector -> node runs -> services call record_token_usage -> wrapper harvests tokens -> appends to JSONL
</verification>

<success_criteria>
- node_wrapper decorator handles async nodes, captures timing/state/tokens/errors
- All 8 LLM call sites in services inject token usage via record_token_usage
- build_graph wraps all 7 nodes with node_wrapper
- Observability failures never interrupt pipeline execution
- Graph still compiles and the node function signatures remain compatible with LangGraph
</success_criteria>

<output>
After completion, create `.planning/phases/10-pipeline-observability-backend/10-02-SUMMARY.md`
</output>
